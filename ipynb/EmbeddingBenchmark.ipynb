{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word nums : 31388\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ref_file = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.40000.response'\n",
    "embed_file = '/ldev/tensorflow/nmt2/word_embed/wiki.zh.vec'\n",
    "eval_files = [\n",
    "    '/ldev/tensorflow/nmt2/nmt/infer_test/RL0.test.txt',\n",
    "    '/ldev/tensorflow/nmt2/nmt/beam_search/Subword2_10_test_f.inf.response',\n",
    "    '/ldev/tensorflow/nmt2/nmt/beam_search/word_4W_10_test_f.inf.response',\n",
    "    '/ldev/tensorflow/nmt2/nmt/beam_search/cnnEncDec_10_test_f.inf.response',\n",
    "\n",
    "]\n",
    "eval_subword = ['hybrid','bpe','','char2word',None]\n",
    "\n",
    "# 统计所有用到的词\n",
    "word_dict = set()\n",
    "\n",
    "def encode(sentence, subword_option_1):\n",
    "    sentence = sentence.strip('\\n').lower()\n",
    "    if subword_option_1 == 'bpe':\n",
    "        sentence = re.sub(\"@@ \", \"\", sentence)\n",
    "    if subword_option_1 == 'space':\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        sentence = sentence.replace(\"<space>\",\" \")\n",
    "    if subword_option_1 == 'char':\n",
    "        sentence = sentence.replace(\"<space>\", \"\")\n",
    "        sentence = sentence.replace(\"@@\", \"\")\n",
    "        sentence = sentence.replace(\" \",\"\")\n",
    "        sentence = \" \".join(sentence)\n",
    "    elif subword_option_1 == 'char2char':\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        sentence = sentence.replace(\"@@\", \"\")\n",
    "        sentence = \" \".join(sentence)\n",
    "    elif subword_option_1 == 'char2word':\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        sentence = sentence.replace(\"@@\", \" \")\n",
    "        # sentence = \" \".join(sentence)\n",
    "    elif subword_option_1 == 'hybrid':\n",
    "        sentence = sentence.replace(\" @@ \", \"\")\n",
    "        sentence = sentence.replace(\"@@ \", \"\")\n",
    "        sentence = sentence.replace(\" @@\", \"\")\n",
    "    elif subword_option_1 == 'hybrid2':\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        sentence = sentence.replace(\"@@\", \" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "for file,subword in zip(eval_files + [ref_file], eval_subword +[None]):\n",
    "    with open(file, 'r', encoding = 'utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            encoded_line = encode(line,subword)\n",
    "            for word in encoded_line.split(' '):\n",
    "                word_dict.add(word.lower())\n",
    "\n",
    "print('word nums : %d' % (len(word_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(embed_file,encoding='ISO-8859-1') as f:\n",
    "#     with open(embed_file+'.u8','w+',encoding='utf-8') as f2:\n",
    "#         for line in f.readlines():\n",
    "#             try:\n",
    "#                 uft_str = line.encode(\"iso-8859-1\").decode('utf-8') \n",
    "#                 f2.write(uft_str)\n",
    "#             except :\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15198\n"
     ]
    }
   ],
   "source": [
    "# 加载embedding\n",
    "import numpy as np\n",
    "def read_vectors(path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "    lines_num, dim = 0, 0\n",
    "    vectors = {}\n",
    "    iw = []\n",
    "    wi = {}\n",
    "    with open(path,encoding='utf-8') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.strip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            if tokens[0] in word_dict:\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "                vectors[tokens[0]] = vectors[tokens[0]]\n",
    "                iw.append(tokens[0])\n",
    "            if topn != 0 and lines_num >= topn:\n",
    "                break\n",
    "    for i, w in enumerate(iw):\n",
    "        wi[w] = i\n",
    "    return vectors, iw, wi, dim\n",
    "\n",
    "vectors = read_vectors(embed_file,0)\n",
    "print(len(vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ldev/tensorflow/nmt2/nmt/infer_test/RL0.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/Subword2_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/word_4W_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/cnnEncDec_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.40000.response\n",
      "[[0.001041069137744208, 0.0010710551323975907, 0.000797152680494999, 0.0005561805227645554, 0.9999999808640893]]\n"
     ]
    }
   ],
   "source": [
    "# 计算一个文件的embedding\n",
    "random_embedding = dict()\n",
    "stop_words = set(['！','？','。','.','，'])\n",
    "def get_embeddings(file,subword=None):\n",
    "    embeddings = []\n",
    "    valids = []\n",
    "    count = 0\n",
    "    with open(file, encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            encoded_line = encode(line,subword)\n",
    "            tmp = np.zeros([300])\n",
    "            count = 0\n",
    "            for word in encoded_line.split(' '):\n",
    "                if word in vectors[0]:\n",
    "                    tmp += vectors[0][word]\n",
    "                    count += 1\n",
    "                else:\n",
    "                    if word in random_embedding:\n",
    "                        noisy = random_embedding[word]\n",
    "                    else:  \n",
    "                        noisy = np.random.normal(size=[300])\n",
    "                        random_embedding[word] = noisy\n",
    "                    tmp+=noisy\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                tmp = tmp / sum(np.sqrt(tmp*tmp))\n",
    "                valids.append(1)\n",
    "            else:\n",
    "                valids.append(0)\n",
    "            embeddings.append(tmp)\n",
    "        return embeddings,valids\n",
    "ref_embeddings  = get_embeddings(ref_file,None)\n",
    "def distances(embedA,embedB,validA,validB):\n",
    "    res = []\n",
    "    \n",
    "    for a,b,c,d in zip(embedA,embedB,validA,validB):\n",
    "        dis = (sum(a*b)) / (np.sqrt(sum(a*a))* np.sqrt(sum(b*b))+1e-10)\n",
    "        res.append(dis)\n",
    "        \n",
    "    return sum(res) / len(res)\n",
    "embeddings = []\n",
    "final_names = eval_files + [ref_file]\n",
    "valids = []\n",
    "for file,subword in zip(eval_files + [ref_file], eval_subword +[None]):\n",
    "    print(file)\n",
    "    embedding,valid = get_embeddings(file,subword)\n",
    "    embeddings.append(embedding)\n",
    "    valids.append(valid)\n",
    "\n",
    "matrix = []\n",
    "for i in range(len(embeddings)-1,len(embeddings)):\n",
    "    row = []\n",
    "    for j in range(0,len(embeddings)):\n",
    "        diss = distances(embeddings[i],embeddings[j],valids[i],valids[j])\n",
    "        row.append(diss)\n",
    "    matrix.append(row)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ldev/tensorflow/nmt2/nmt/infer_test/RL0.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/Subword2_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/word_4W_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/char_raw_10_test_f.inf.response\n",
      "/ldev/tensorflow/nmt2/nmt/beam_search/cnnEncDec_10_test_f.inf.response\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f0ebeb7eb611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mref_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_subword\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mvalids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f0ebeb7eb611>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(file, subword)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 计算一个文件的embedding Gready\n",
    "# def get_embeddings(file,subword=None):\n",
    "#     embeddings = []\n",
    "#     valids = []\n",
    "#     count = 0\n",
    "#     with open(file, encoding='utf-8') as fin:\n",
    "#         for line in fin.readlines():\n",
    "#             encoded_line = encode(line,subword)\n",
    "#             tmp = []\n",
    "#             count = 0\n",
    "#             for word in encoded_line.split(' '):\n",
    "#                 if word in vectors[0]:\n",
    "#                     tmp.append(vectors[0][word])\n",
    "#                     count += 1\n",
    "#                 else:\n",
    "#                     noisy = np.random.uniform(size=[300])\n",
    "#                     noisy = noisy/sum(np.sqrt(noisy*noisy))\n",
    "#                     tmp.append(noisy)\n",
    "#                     count += 1\n",
    "#             if count > 0:\n",
    "#                 valids.append(1)\n",
    "#             else:\n",
    "#                 valids.append(0)\n",
    "#             embeddings.append(tmp)\n",
    "#         return embeddings,valids\n",
    "# ref_embeddings  = get_embeddings(ref_file,None)\n",
    "\n",
    "# def distances(embedA,embedB,validA,validB):\n",
    "#     res = []\n",
    "    \n",
    "#     for a,b,c,d in zip(embedA,embedB,validA,validB):\n",
    "#         if c*d == 0:\n",
    "#             print('error')\n",
    "#         for r in a:\n",
    "#             score1 = -1\n",
    "#             for r1 in b:\n",
    "#                 dis = (sum(r*r1)+1e-10) / (np.sqrt(sum(r*r))* np.sqrt(sum(r1*r1))+1e-10)\n",
    "#                 score1 = max(dis,score1)\n",
    "#         for r in b:\n",
    "#             score2 = -1\n",
    "#             for r1 in a:\n",
    "#                 dis = (sum(r*r1)+1e-10) / (np.sqrt(sum(r*r))* np.sqrt(sum(r1*r1))+1e-10)\n",
    "#                 score2 = max(dis,score2)\n",
    "#         res.append(score1+score2)\n",
    "        \n",
    "#     return sum(res) / len(res) / 2\n",
    "# embeddings = []\n",
    "# final_names = eval_files + [ref_file]\n",
    "# valids = []\n",
    "# for file,subword in zip(eval_files + [ref_file], eval_subword +[None]):\n",
    "#     print(file)\n",
    "#     embedding,valid = get_embeddings(file,subword)\n",
    "#     embeddings.append(embedding)\n",
    "#     valids.append(valid)\n",
    "\n",
    "# matrix = []\n",
    "# for i in range(len(embeddings)-1,len(embeddings)):\n",
    "#     row = []\n",
    "#     for j in range(0,len(embeddings)):\n",
    "#         diss = distances(embeddings[i],embeddings[j],valids[i],valids[j])\n",
    "#         row.append(diss)\n",
    "#     matrix.append(row)\n",
    "# print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ldev/tensorflow/nmt2/nmt/infer_test/cn_albpe2.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/infer_test/cn_rnn_hlat.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/infer_test/bpe_lstm.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/infer_test/cnnencdec.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/infer_test/chinese_lstm.test.txt\n",
      "/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.40000.response\n",
      "[[0.5710198406937087, 0.5525693729637218, 0.5578741953815899, 0.5658481167931326, 0.3565961653136164, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 1.0, 0.9999999999999998]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
