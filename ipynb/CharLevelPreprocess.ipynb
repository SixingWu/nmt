{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 4435959\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "in_path = '/ldev/tensorflow/seq2seq/rawdata/DiaTest/'\n",
    "\n",
    "message_path = in_path + 'huaweiFull.message'\n",
    "response_path = in_path + 'huaweiFull.response'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(message_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    messages = [line.strip('\\n') for line in lines]\n",
    "    for line in messages:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "with open(response_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    responses = [line.strip('\\n') for line in lines]\n",
    "    for line in responses:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "assert len(messages) == len(responses)\n",
    "\n",
    "print('total lines: %d' % (len(messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['看到 榜样 的 力量 ， 大家 心里 会 更 安心',\n",
       " '某 前锋 完全 没有 预判 跑位 ， 速度 还 没有 对手 后卫 转身 加 回 追 速度快',\n",
       " '它 这 是 间接 暗示 你 ， 不如 搂 在一起 睡 会儿 吧',\n",
       " 'I want to go to Yunnan province in 2013 .',\n",
       " '如果 雍正 那会儿 有 QQ ， 那 《 甄嬛传 》 里边 的 人 … …',\n",
       " '这个 是 日本 的 那个 什么 组合么',\n",
       " '位置 是 创意 的 根源 ， 二师兄 ， 哈哈 。',\n",
       " '我 爱上 了 一 匹 汗 血 宝马 ， 可是 我 没有 马场 也 没有 钱 。',\n",
       " '这个 。 掉 节操 的 问句 ， 床上 咋办 。',\n",
       " '要么 带 套 ， 要么 大 肚 。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "看到 榜样 的 力量 ， 大家 心里 会 更 安心\n",
      "看到 榜样 的 力量 ， 大家 心里 会 更 安心#\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter(text):\n",
    "    res = re.sub(r'#[\\S ]*#',\"\", text)\n",
    "    res = re.sub(r'（[\\S ]+）',\"\", text)\n",
    "    res = re.sub(r'alink',\"\", res)\n",
    "    res = re.sub(r'[ ]+',\" \", res)\n",
    "    return res.strip()\n",
    "\n",
    "\n",
    "for line in responses[1000:1001]:\n",
    "    print(line)\n",
    "    print(filter(line) + '#') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3788600\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "new_messages = []\n",
    "new_responses = []\n",
    "msg_counter = defaultdict(int)\n",
    "res_counter = defaultdict(int)\n",
    "str_counter = defaultdict(int)\n",
    "for msg,res in zip(messages,responses):\n",
    "    if msg == res:\n",
    "        continue\n",
    "    fmsg = filter(msg)\n",
    "    fres = filter(res)\n",
    "    vt = fmsg +'alink' + fres\n",
    "    if msg_counter[fmsg] > 75 or res_counter[fres] > 75 or str_counter[vt] > 20 :\n",
    "        continue\n",
    "    if fmsg == fres:\n",
    "        continue\n",
    "    msg_counter[fmsg] += 1\n",
    "    res_counter[fres]  += 1\n",
    "    str_counter[vt] += 1\n",
    "    \n",
    "    nmsg = ' '.join( x for x in fmsg.split())\n",
    "    nres = ' '.join( x for x in fres.split())\n",
    "    fmsg = fmsg.split()\n",
    "    fres = fres.split()\n",
    "    if len(fmsg) < 2 or len(fres) < 2 or len(fmsg) > 20 or len(fres) > 20:\n",
    "        continue\n",
    "    new_messages.append(nmsg.split())\n",
    "    new_responses.append(nres.split())\n",
    "print('total lines: %d' % (len(new_messages))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_messages = []\n",
    "seg_responses = []\n",
    "for line in new_messages:\n",
    "    segs = []\n",
    "    for item in line:\n",
    "        if item == '@@':\n",
    "            segs.append(item)\n",
    "        else:\n",
    "            segs = segs + list(item)\n",
    "    seg_messages.append(segs)\n",
    "\n",
    "for line in new_responses:\n",
    "    segs = []\n",
    "    for item in line:\n",
    "        if item == '@@':\n",
    "            segs.append(item)\n",
    "        else:\n",
    "            segs = segs + list(item)\n",
    "    seg_responses.append(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['中', '国', '移', '动', '营', '销', '行', '来', '发', '展', '报', '告'], ['小', '马', '也', '疯', '狂', '-', '-', '-', '-', '-', '-', '地', '位', '之', '争', '。'], ['那', '些', '年', '，', '我', '们', '一', '起', '偷', '看', '过', '的', '电', '视', '。', '「', '暴', '走', '漫', '画', '」'], ['北', '京', '的', '小', '纯', '洁', '们', '，', '周', '日', '见', '。', '#', '硬', '汉', '摆', '拍', '清', '纯', '照', '#'], ['要', '是', '这', '一', '年', '哭', '泣', '的', '理', '由', '不', '再', '是', '难', '过', '而', '是', '感', '动', '会', '多', '么', '好'], ['对', '于', '国', '内', '动', '漫', '画', '作', '者', '引', '用', '工', '笔', '素', '材', '的', '一', '些', '个', '人', '意', '见', '。'], ['猫', '咪', '保', '镖', '最', '赞', '了', '！', '你', '们', '看', '懂', '了', '吗', '？', '！'], ['莫', '愁', '和', '所', '有', '人', '开', '了', '一', '个', '玩', '笑', '—', '—', '其', '实', '，', '她', '是', '会', '正', '常', '唱', '歌', '的', '…', '…'], ['你', '见', '过', '皮', '卡', '丘', '喝', '水', '的', '样', '子', '吗', '？'], ['如', '果', '有', '个', '人', '能', '让', '你', '忘', '掉', '过', '去', '，', '那', 'T', 'A', '很', '可', '能', '就', '是', '你', '的', '未', '来', '。'], ['我', '在', '北', '京', '，', '2', '4', '岁', '，', '想', '去', '马', '尔', '代', '夫', '，', '一', '个', '人', '。'], ['哥', '你', '还', '跳', '不', '跳', '楼', '了', '？', '我', '们', '要', '下', '班', '啊', '！'], ['龙', '生', '龙', '，', '凤', '生', '凤', '，', '是', '个', '喵', '咪', '它', '就', '萌', '。'], ['从', '胚', '胎', '期', '开', '始', '的', '面', '部', '特', '征', '演', '变', '过', '程'], ['本', '届', '轮', '值', '主', '席', '王', '石', '致', '开', '幕', '词', '。', '讲', '6', '0', '岁', '上', '哈', '佛', '。'], ['非', '常', '不', '喜', '欢', '北', '京', '现', '在', '的', '天', '气', '…', '…', '非', '常', '…', '…'], ['我', '第', '一', '次', '坐', '飞', '机', '是', '进', '安', '达', '信', '的', '入', '职', '培', '训', '，', '在', '深', '圳', '。', '你', '们', '哪', '？'], ['人', '生', '如', '戏', '，', '全', '靠', '演', '技', '。', '小', '受', '吓', '坏', '了', '。'], ['为', '什', '么', '这', '世', '上', '会', '有', '人', '以', '刁', '难', '他', '人', '为', '乐', '呢', '？'], ['算', '了', '算', '了', '，', '我', '看', '出', '来', '了', '，', '你', '们', '都', '想', '看', '男', '人', '！', '上', '张', '美', '男', '图', '。']]\n"
     ]
    }
   ],
   "source": [
    "print(seg_messages[0:20])\n",
    "new_messages = seg_messages\n",
    "new_responses = seg_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.00\n",
      "3 0.00\n",
      "4 0.00\n",
      "5 0.00\n",
      "6 0.00\n",
      "7 0.00\n",
      "8 0.01\n",
      "9 0.07\n",
      "10 0.13\n",
      "11 0.20\n",
      "12 0.27\n",
      "13 0.33\n",
      "14 0.39\n",
      "15 0.45\n",
      "16 0.51\n",
      "17 0.56\n",
      "18 0.61\n",
      "19 0.66\n",
      "20 0.70\n",
      "21 0.75\n",
      "22 0.79\n",
      "23 0.82\n",
      "24 0.86\n",
      "25 0.89\n",
      "26 0.92\n",
      "27 0.94\n",
      "28 0.96\n",
      "29 0.98\n",
      "30 0.98\n",
      "31 0.99\n",
      "32 0.99\n",
      "33 0.99\n",
      "34 0.99\n",
      "35 0.99\n",
      "36 1.00\n",
      "37 1.00\n",
      "38 1.00\n",
      "39 1.00\n",
      "40 1.00\n",
      "41 1.00\n",
      "42 1.00\n",
      "43 1.00\n",
      "44 1.00\n",
      "45 1.00\n",
      "46 1.00\n",
      "47 1.00\n",
      "48 1.00\n",
      "49 1.00\n",
      "50 1.00\n",
      "51 1.00\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "60 1.00\n",
      "61 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "67 1.00\n",
      "68 1.00\n",
      "69 1.00\n",
      "70 1.00\n",
      "71 1.00\n",
      "72 1.00\n",
      "73 1.00\n",
      "74 1.00\n",
      "75 1.00\n",
      "76 1.00\n",
      "77 1.00\n",
      "78 1.00\n",
      "79 1.00\n",
      "82 1.00\n",
      "83 1.00\n",
      "char_nums_per_word:\n",
      "1 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = new_messages\n",
    "tokenized_responses = new_responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/charlevel/'\n",
    "import random\n",
    "random.seed(6666)\n",
    "\n",
    "total_num = len(messages)\n",
    "dev_num =  20000\n",
    "test_num = 20000\n",
    "max_len = 30\n",
    "train_num = total_num - dev_num - test_num\n",
    "\n",
    "random_orders = range(0, total_num)\n",
    "messages = tokenized_messages\n",
    "responses =  tokenized_responses\n",
    "file_map = {'message':tokenized_messages, 'response':tokenized_responses}\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i][0:max_len]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i][0:max_len]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i][0:max_len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10705\n",
      "6672\n",
      "10540\n",
      "[('，', 4756132), ('的', 3989835), ('。', 3599561), ('！', 2090294), ('是', 2016854), ('了', 1976490), ('不', 1657795), ('一', 1637827), ('我', 1466641), ('这', 1339725), ('有', 1219521), ('你', 1124758), ('人', 1117423), ('？', 1115009), ('个', 994458), ('好', 894553), ('么', 839424), ('看', 737735), ('在', 734036), ('大', 714054), ('来', 634469), ('…', 627389), ('就', 620277), ('啊', 605492), ('天', 597183), ('要', 550680), ('都', 546573), ('上', 523847), ('到', 489755), ('哈', 480373), ('#', 469340), ('还', 456149), ('们', 450280), ('子', 444647), ('会', 443480), ('小', 436917), ('没', 430028), ('中', 410926), ('时', 406794), ('真', 404760), ('得', 400609), ('可', 389181), ('说', 387806), ('过', 382557), ('生', 381387), ('最', 374117), ('也', 373604), ('国', 373210), ('能', 372492), ('家', 370674), ('想', 370388), ('下', 368386), ('多', 368216), ('很', 363353), ('年', 353686), ('那', 343895), ('心', 340691), ('0', 340371), ('样', 338569), ('为', 335915), ('去', 328149), ('1', 326782), ('爱', 321009), ('吧', 316864), ('什', 315509), ('自', 314161), ('以', 313763), ('e', 309859), ('o', 304370), ('太', 303560), ('吗', 293742), ('i', 291819), ('出', 290388), ('：', 278803), ('a', 276737), ('发', 276394), ('美', 275421), ('点', 274994), ('后', 273640), ('老', 271912), ('2', 270960), ('里', 258423), ('和', 258245), ('转', 246193), ('开', 245305), ('图', 240056), ('吃', 239991), ('今', 231896), ('起', 229826), ('用', 226218), ('道', 222535), ('现', 220353), ('对', 216411), ('日', 216110), ('手', 213990), ('感', 213090), ('喜', 212514), ('学', 212456), ('新', 212426), ('如', 211770)]\n"
     ]
    }
   ],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/charlevel/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/charlevel/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "print(len(sorted_counter))\n",
    "print(len(sorted_message_counter))\n",
    "print(len(sorted_response_counter))\n",
    "print(sorted_counter[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/charlevel/'\n",
    "for file in ['message','response']:\n",
    "    with open(out_path + 'vocab.'+file,'w+',encoding='utf-8') as fout:\n",
    "        #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "        for item in sorted_counter:\n",
    "            fout.write(item[0]+'\\n')\n",
    "with open(out_path + 'vocab.'+'separate.response','w+',encoding='utf-8') as fout:\n",
    "        #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "        for item in sorted_response_counter:\n",
    "            fout.write(item[0]+'\\n')\n",
    "with open(out_path + 'vocab.'+'separate.message','w+',encoding='utf-8') as fout:\n",
    "        #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "        for i,item in enumerate(sorted_message_counter):\n",
    "            if len(item[0]) == 0:\n",
    "                print('flag '+str(i))\n",
    "            fout.write(item[0]+'\\n')\n",
    "\n",
    "with open(out_path + 'vocab.'+'.counter','w+',encoding='utf-8') as fout:\n",
    "        fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "        for item in sorted_counter:\n",
    "            fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6672\n",
      "0 0.0366\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
