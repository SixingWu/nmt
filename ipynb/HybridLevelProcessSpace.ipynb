{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取所有Chracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6688\n",
      "120929\n",
      "10549\n",
      "618079\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "\n",
    "words_dict = {}\n",
    "chars_dict = {}\n",
    "for file in ['message','response']:\n",
    "    characters = defaultdict(int)\n",
    "    words = defaultdict(int)\n",
    "    with open(in_path+'train.'+file,'r',encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            tokens = line.strip('\\n').split()\n",
    "            for token in tokens:\n",
    "                words[token] += 1\n",
    "                for char in token:\n",
    "                    characters[char] += 1\n",
    "        print(len(characters))\n",
    "        print(len(words))\n",
    "        words_dict[file] = words\n",
    "        chars_dict[file] = characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写一个程序，控制Character的数量\n",
    "Character 包含双份的，即包含未终结的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c56fd2cfc48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mcount_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mcount_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcount_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c56fd2cfc48d>\u001b[0m in \u001b[0;36mcount_message\u001b[0;34m(total_count, file, output_id)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchars_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_dict' is not defined"
     ]
    }
   ],
   "source": [
    "def count_response(total_count,file,output_id):\n",
    "    words = words_dict[file]\n",
    "    chars = (chars_dict[file].keys())\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    print('response char nums: %d' % len(chars) )\n",
    "    for i in range(0,total_count - len(chars)):\n",
    "        vocab.add(sorted_words[i][0])\n",
    "    for char in chars:\n",
    "            vocab.add(char)\n",
    "    i = 0\n",
    "    while(len(vocab) < total_count):\n",
    "        vocab.add(sorted_words[i][0])\n",
    "        i += 1\n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(['@@']+list(vocab)))\n",
    "    print(\"response : %d\" % len(vocab))\n",
    "\n",
    "def count_message(total_count,file,output_id):\n",
    "    words = words_dict[file]\n",
    "    chars = (chars_dict[file].keys())\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    print('message char nums: %d' % len(chars) )\n",
    "    for i in range(total_count-len(chars)//4):\n",
    "        vocab.add(sorted_words[i][0])   \n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(list(vocab)))\n",
    "    print(\"message : %d\" % len(vocab))\n",
    "\n",
    "\n",
    "count_message(40000,'message',40000)\n",
    "count_response(40000,'response',40000)\n",
    "count_message(20000,'message',20000)\n",
    "count_response(20000,'response',20000)\n",
    "count_message(80000,'message',80000)\n",
    "count_response(80000,'response',80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取对应的文件，然后生成相应的混合表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3748600\n",
      "20000\n",
      "20000\n",
      "3748600\n",
      "20000\n",
      "20000\n",
      "3748600\n",
      "20000\n",
      "20000\n",
      "3748600\n",
      "20000\n",
      "20000\n",
      "3748600\n",
      "20000\n",
      "20000\n",
      "3748600\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "for file in ['response']:\n",
    "    for vocab_size in [20000,40000,80000]:\n",
    "        vocab = set()\n",
    "        with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/'+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                            \n",
    "                            if token not in vocab:\n",
    "                                for char in token[:-1]:\n",
    "                                    res.append(char)\n",
    "                                    res.append('@@')\n",
    "                                res.append(token[-1])\n",
    "                                    \n",
    "                            else:\n",
    "                                res.append(token)\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "                    print(total_len)\n",
    "            \n",
    "            if prefix == 'traind':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)\n",
    "\n",
    "for file in ['message']:\n",
    "    for vocab_size in [20000,40000,80000]:\n",
    "        vocab = set()\n",
    "        with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/'+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                                res.append(token)\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "                    print(total_len)\n",
    "            \n",
    "            if prefix == 'traind':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 6690\n",
      "new num: 6690\n",
      "original num: 3748600\n",
      "new num: 10552\n",
      "original num: 10552\n",
      "new num: 10552\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "seg_len = 5\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [40000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 999\n",
    "            vocab[seg_pad] = 999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "            \n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        nfile_path = file_path\n",
    "        with open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.message' % (vocab), r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.%d.message' % vocab)\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.response'% (vocab), r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.%d.response'% vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/test.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/test.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/dev.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/dev.%d.response'% (vocab), seg_len,vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
