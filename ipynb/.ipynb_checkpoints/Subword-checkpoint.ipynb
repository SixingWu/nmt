{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 4435959\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/Subword/'\n",
    "\n",
    "message_path = in_path + 'huaweiFull.message'\n",
    "response_path = in_path + 'huaweiFull.response'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(message_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    messages = [line.strip('\\n') for line in lines]\n",
    "    for line in messages:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "with open(response_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    responses = [line.strip('\\n') for line in lines]\n",
    "    for line in responses:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "assert len(messages) == len(responses)\n",
    "\n",
    "print('total lines: %d' % (len(messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['看到 榜样 的 力量 ， 大家 心里 会 更 安心',\n",
       " '某 前锋 完全 没有 预判 跑位 ， 速度 还 没有 对手 后卫 转身 加 回 追 速度快',\n",
       " '它 这 是 间接 暗示 你 ， 不如 搂 在一起 睡 会儿 吧',\n",
       " 'I want to go to Yunnan province in 2013 .',\n",
       " '如果 雍正 那会儿 有 QQ ， 那 《 甄嬛传 》 里边 的 人 … …',\n",
       " '这个 是 日本 的 那个 什么 组合么',\n",
       " '位置 是 创意 的 根源 ， 二师兄 ， 哈哈 。',\n",
       " '我 爱上 了 一 匹 汗 血 宝马 ， 可是 我 没有 马场 也 没有 钱 。',\n",
       " '这个 。 掉 节操 的 问句 ， 床上 咋办 。',\n",
       " '要么 带 套 ， 要么 大 肚 。']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/Subword/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/Subword/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').replace('@@ ',' @@ ').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39332\n",
      "43982\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_message_counter))\n",
    "print(len(sorted_response_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_nums = [60000]\n",
    "for vocab_num in vocab_nums:\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/Subword/'\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39332\n",
      "0 0.0525\n",
      "10000 0.9088\n",
      "20000 0.9644\n",
      "30000 0.9909\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n",
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "seg_len = 5\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [20000,40000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 999\n",
    "            vocab[seg_pad] = 999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 20:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        if file_path[-len('message'):] == 'message':\n",
    "            nfile_path = file_path[0:-len('message')]+''+str(vocab)+'.message'\n",
    "        else:\n",
    "            nfile_path = file_path[0:-len('response')]+''+str(vocab)+'.response'\n",
    "        with open(nfile_path , 'w+', encoding='utf-8') as foout,open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                foout.write(line)\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.message' % vocab)\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.response'% vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.response', seg_len,vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
