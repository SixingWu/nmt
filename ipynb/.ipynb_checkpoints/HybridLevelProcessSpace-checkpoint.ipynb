{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取所有Chracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6688\n",
      "120929\n",
      "10549\n",
      "618079\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "\n",
    "words_dict = {}\n",
    "chars_dict = {}\n",
    "for file in ['message','response']:\n",
    "    characters = defaultdict(int)\n",
    "    words = defaultdict(int)\n",
    "    with open(in_path+'train.'+file,'r',encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            tokens = line.strip('\\n').split()\n",
    "            for token in tokens:\n",
    "                words[token] += 1\n",
    "                for char in token:\n",
    "                    characters[char] += 1\n",
    "        print(len(characters))\n",
    "        print(len(words))\n",
    "        words_dict[file] = words\n",
    "        chars_dict[file] = characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写一个程序，控制Character的数量\n",
    "Character 包含双份的，即包含未终结的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char nums: 6688\n",
      "message : 38328\n",
      "response : 40000\n",
      "char nums: 6688\n",
      "message : 18328\n",
      "response : 20000\n",
      "char nums: 6688\n",
      "message : 78328\n",
      "response : 80000\n"
     ]
    }
   ],
   "source": [
    "def count_response(total_count,file,output_id):\n",
    "    words = words_dict[file]\n",
    "    chars = (chars_dict[file].keys())\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    for i in range(0,total_count - len(chars)):\n",
    "        vocab.add(sorted_words[i][0])\n",
    "    for char in chars:\n",
    "            vocab.add(char)\n",
    "    i = 0\n",
    "    while(len(vocab) < total_count):\n",
    "        vocab.add(sorted_words[i][0])\n",
    "        i += 1\n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(['@@']+list(vocab)))\n",
    "    print(\"response : %d\" % len(vocab))\n",
    "\n",
    "def count_message(total_count,file,output_id):\n",
    "    words = words_dict[file]\n",
    "    chars = (chars_dict[file].keys())\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    print('char nums: %d' % len(chars) )\n",
    "    for i in range(total_count-len(chars)//4):\n",
    "        vocab.add(sorted_words[i][0])   \n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(list(vocab)))\n",
    "    print(\"message : %d\" % len(vocab))\n",
    "\n",
    "\n",
    "count_message(40000,'message',40000)\n",
    "count_response(40000,'response',40000)\n",
    "count_message(20000,'message',20000)\n",
    "count_response(20000,'response',20000)\n",
    "count_message(80000,'message',80000)\n",
    "count_response(80000,'response',80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取对应的文件，然后生成相应的混合表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_per_sentence: response 20000\n",
      "2 0.00 3 0.00 4 0.00 5 0.01 6 0.06 7 0.13 8 0.21 9 0.30 10 0.39 11 0.48 12 0.55 13 0.62 14 0.68 15 0.73 16 0.77 17 0.81 18 0.85 19 0.88 20 0.90 21 0.92 22 0.94 23 0.95 24 0.96 25 0.97 26 0.97 27 0.98 28 0.98 29 0.98 30 0.99 31 0.99 32 0.99 33 0.99 34 0.99 35 0.99 36 0.99 37 0.99 38 1.00 39 1.00 40 1.00 41 1.00 42 1.00 43 1.00 44 1.00 45 1.00 46 1.00 47 1.00 48 1.00 49 1.00 50 1.00 51 1.00 52 1.00 53 1.00 54 1.00 55 1.00 56 1.00 57 1.00 58 1.00 59 1.00 60 1.00 61 1.00 62 1.00 63 1.00 64 1.00 65 1.00 66 1.00 67 1.00 68 1.00 69 1.00 70 1.00 71 1.00 72 1.00 73 1.00 74 1.00 75 1.00 76 1.00 77 1.00 78 1.00 79 1.00 80 1.00 81 1.00 82 1.00 83 1.00 84 1.00 85 1.00 86 1.00 87 1.00 88 1.00 89 1.00 90 1.00 91 1.00 92 1.00 93 1.00 94 1.00 95 1.00 96 1.00 97 1.00 98 1.00 99 1.00 100 1.00 101 1.00 102 1.00 103 1.00 104 1.00 105 1.00 106 1.00 107 1.00 108 1.00 109 1.00 110 1.00 111 1.00 112 1.00 113 1.00 114 1.00 115 1.00 116 1.00 117 1.00 118 1.00 119 1.00 120 1.00 121 1.00 122 1.00 123 1.00 124 1.00 125 1.00 127 1.00 128 1.00 129 1.00 130 1.00 131 1.00 132 1.00 133 1.00 134 1.00 135 1.00 136 1.00 137 1.00 138 1.00 139 1.00 140 1.00 141 1.00 142 1.00 143 1.00 146 1.00 156 1.00 160 1.00 161 1.00 \n",
      "items_per_sentence: response 40000\n",
      "2 0.00 3 0.00 4 0.00 5 0.02 6 0.09 7 0.18 8 0.29 9 0.39 10 0.48 11 0.56 12 0.64 13 0.70 14 0.75 15 0.80 16 0.84 17 0.87 18 0.90 19 0.93 20 0.95 21 0.96 22 0.97 23 0.97 24 0.98 25 0.98 26 0.99 27 0.99 28 0.99 29 0.99 30 0.99 31 0.99 32 0.99 33 0.99 34 1.00 35 1.00 36 1.00 37 1.00 38 1.00 39 1.00 40 1.00 41 1.00 42 1.00 43 1.00 44 1.00 45 1.00 46 1.00 47 1.00 48 1.00 49 1.00 50 1.00 51 1.00 52 1.00 53 1.00 54 1.00 55 1.00 56 1.00 57 1.00 58 1.00 59 1.00 60 1.00 61 1.00 62 1.00 63 1.00 64 1.00 65 1.00 66 1.00 67 1.00 68 1.00 69 1.00 70 1.00 71 1.00 72 1.00 73 1.00 74 1.00 75 1.00 76 1.00 77 1.00 78 1.00 79 1.00 80 1.00 81 1.00 82 1.00 83 1.00 84 1.00 85 1.00 86 1.00 87 1.00 88 1.00 89 1.00 90 1.00 91 1.00 92 1.00 93 1.00 94 1.00 95 1.00 96 1.00 97 1.00 98 1.00 99 1.00 100 1.00 101 1.00 102 1.00 103 1.00 104 1.00 105 1.00 106 1.00 108 1.00 109 1.00 110 1.00 111 1.00 112 1.00 113 1.00 114 1.00 115 1.00 116 1.00 117 1.00 118 1.00 121 1.00 122 1.00 123 1.00 124 1.00 125 1.00 127 1.00 128 1.00 129 1.00 130 1.00 131 1.00 132 1.00 133 1.00 134 1.00 135 1.00 136 1.00 138 1.00 139 1.00 140 1.00 141 1.00 142 1.00 143 1.00 146 1.00 156 1.00 160 1.00 161 1.00 \n",
      "items_per_sentence: response 80000\n",
      "2 0.00 3 0.00 4 0.00 5 0.03 6 0.11 7 0.22 8 0.33 9 0.44 10 0.53 11 0.61 12 0.68 13 0.74 14 0.79 15 0.84 16 0.87 17 0.91 18 0.93 19 0.95 20 0.97 21 0.98 22 0.98 23 0.99 24 0.99 25 0.99 26 0.99 27 0.99 28 0.99 29 0.99 30 1.00 31 1.00 32 1.00 33 1.00 34 1.00 35 1.00 36 1.00 37 1.00 38 1.00 39 1.00 40 1.00 41 1.00 42 1.00 43 1.00 44 1.00 45 1.00 46 1.00 47 1.00 48 1.00 49 1.00 50 1.00 51 1.00 52 1.00 53 1.00 54 1.00 55 1.00 56 1.00 57 1.00 58 1.00 59 1.00 60 1.00 61 1.00 62 1.00 63 1.00 64 1.00 65 1.00 66 1.00 67 1.00 68 1.00 69 1.00 70 1.00 71 1.00 72 1.00 73 1.00 74 1.00 75 1.00 76 1.00 77 1.00 78 1.00 79 1.00 80 1.00 81 1.00 82 1.00 83 1.00 84 1.00 85 1.00 86 1.00 87 1.00 88 1.00 89 1.00 90 1.00 91 1.00 92 1.00 93 1.00 94 1.00 95 1.00 96 1.00 97 1.00 98 1.00 99 1.00 100 1.00 101 1.00 102 1.00 103 1.00 104 1.00 105 1.00 106 1.00 107 1.00 108 1.00 109 1.00 110 1.00 111 1.00 112 1.00 113 1.00 114 1.00 116 1.00 117 1.00 118 1.00 121 1.00 123 1.00 125 1.00 127 1.00 128 1.00 129 1.00 130 1.00 131 1.00 132 1.00 133 1.00 134 1.00 135 1.00 136 1.00 138 1.00 139 1.00 143 1.00 146 1.00 156 1.00 160 1.00 161 1.00 \n",
      "items_per_sentence: message 20000\n",
      "2 0.00 3 0.00 4 0.00 5 0.01 6 0.03 7 0.07 8 0.14 9 0.22 10 0.31 11 0.39 12 0.49 13 0.57 14 0.66 15 0.74 16 0.81 17 0.87 18 0.93 19 0.97 20 1.00 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "for file in ['response']:\n",
    "    for vocab_size in [20000,40000,80000]:\n",
    "        vocab = set()\n",
    "        with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/'+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                            \n",
    "                            if token not in vocab:\n",
    "                                for char in token[:-1]:\n",
    "                                    res.append(char)\n",
    "                                    res.append('@@')\n",
    "                                res.append(token[-1])\n",
    "                                    \n",
    "                            else:\n",
    "                                res.append(token)\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "            \n",
    "            if prefix == 'train':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)\n",
    "\n",
    "for file in ['message']:\n",
    "    for vocab_size in [20000,40000,80000]:\n",
    "        vocab = set()\n",
    "        with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            with open('/ldev/tensorflow/nmt2/nmt/data/hybrid2/'+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                                res.append(token)\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "            \n",
    "            if prefix == 'train':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10552\n",
      "original num: 5573\n",
      "new num: 5573\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "seg_len = 5\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [20000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 999\n",
    "            vocab[seg_pad] = 999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "            \n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        nfile_path = file_path\n",
    "        with open(nfile_path , 'w+', encoding='utf-8') as foout,open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                foout.write(line)\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.message' % (vocab), r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.%d.separate.message' % vocab)\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.response'% (vocab), r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/vocab.%d.separate.response'% vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/train.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/test.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/test.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/dev.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/hybrid2/dev.%d.response'% (vocab), seg_len,vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
