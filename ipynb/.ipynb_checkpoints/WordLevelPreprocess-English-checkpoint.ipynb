{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 2601244\n",
      "total lines: 2601244\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "input_file = '/ldev/tensorflow/nmt2/nmt/data/gitdata/chat_corpus/twitter.big'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(input_file,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    all_lines = [line.strip('\\n') for line in lines]\n",
    "    for i in range(0,len(all_lines),2):\n",
    "        messages.append(all_lines[i])\n",
    "        responses.append(all_lines[i+1])\n",
    "print('total lines: %d' % (len(messages)))\n",
    "print('total lines: %d' % (len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanna bet on it? want the over or under?\n",
      "you’re predicting we’ll know the result precisely 28 minutes after polls close? :-)\n"
     ]
    }
   ],
   "source": [
    "print(responses[88687])\n",
    "print(messages[88687])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paranoia has already began...why do they think shane doesn't wanna work with them? because he's not up their butt?\n",
      "the paranoia has already began ... why do they think shane doesn ' t wanna work with them ? because he ' s not up their butt ?\n",
      "white ford boys 😤😤😂\n",
      "white ford boys 😤😤😂\n",
      "too bad you don't actually go to my school you liar 😂\n",
      "too bad you don ' t actually go to my school you liar 😂\n",
      "before you go to work open each episode up in its own tab\n",
      "before you go to work open each episode up in its own tab\n",
      "would you have sat and waited to see if he had a gun or would you have saved your own life\n",
      "would you have sat and waited to see if he had a gun or would you have saved your own life\n",
      "aww thanks chalupa i know i can trust your taste 👌\n",
      "aww thanks chalupa i know i can trust your taste 👌\n",
      "the nats used to be canadian. try again.\n",
      "the nats used to be canadian . try again .\n",
      ", please, please! the confirm that you will honor this promise:\n",
      ", please , please ! the confirm that you will honor this promise :\n",
      "thank you!!\n",
      "thank you !!\n",
      "keeping my fingers crossed that he still has another ed wood in him before he retires.\n",
      "keeping my fingers crossed that he still has another ed wood in him before he retires .\n",
      "but you already gave jerusalem to israel, and you think n korea shuld have nukes.sounds like world to me.\n",
      "but you already gave jerusalem to israel , and you think n korea shuld have nukes . sounds like world to me .\n",
      "glass or paper straws - preferably no 'straw' waste. ban !\n",
      "glass or paper straws - preferably no ' straw ' waste . ban !\n",
      "thats right the secret is out! has now! classic sitcoms all free2watch! …\n",
      "thats right the secret is out ! has now ! classic sitcoms all free2watch ! …\n",
      "ya omg really how'd you think of that one??\n",
      "ya omg really how ' d you think of that one ??\n",
      "i wouldn't have shot bc he didn't pose as a threat\n",
      "i wouldn ' t have shot bc he didn ' t pose as a threat\n",
      "best of them all.\n",
      "best of them all .\n",
      "willett has been piss poor since the masters and russell knox has been great yet he cannot even make team\n",
      "willett has been piss poor since the masters and russell knox has been great yet he cannot even make team\n",
      "thats right the secret is out! has now! classic sitcoms all free2watch! …\n",
      "thats right the secret is out ! has now ! classic sitcoms all free2watch ! …\n",
      "that meme wasn't a lie. the inclusion of what was left off does not excuse what was revealed.\n",
      "that meme wasn ' t a lie . the inclusion of what was left off does not excuse what was revealed .\n",
      "half cash and private equity? so they got that off of paltry interest rates and, presumably, crazy valuations on their pe?\n",
      "half cash and private equity ? so they got that off of paltry interest rates and , presumably , crazy valuations on their pe ?\n",
      "he was on the blue smurf team!\n",
      "he was on the blue smurf team !\n",
      "qana in 96 was hezbollah firing 2 missiles behind civilians at israel and israel firin…\n",
      "qana in 96 was hezbollah firing 2 missiles behind civilians at israel and israel firin …\n",
      "\"negotiations there are always two sides.\" but in cleveland there's only one side: lebron's. the king at his worst.\n",
      "\" negotiations there are always two sides .\" but in cleveland there ' s only one side : lebron ' s . the king at his worst .\n",
      "i don't even know\n",
      "i don ' t even know\n",
      "rt : you should spread love not war\n",
      "rt : you should spread love not war\n",
      "no sex tape. if you mean her scene on a soap opera pretty sick. it was on tv idiot. she should sue you. i certainly would.\n",
      "no sex tape . if you mean her scene on a soap opera pretty sick . it was on tv idiot . she should sue you . i certainly would .\n",
      "i actually meant a different kind of fist but okay that works too then shiro + you would become one of the muppets lol\n",
      "i actually meant a different kind of fist but okay that works too then shiro + you would become one of the muppets lol\n",
      "i heard he's better than the old esea observer\n",
      "i heard he ' s better than the old esea observer\n",
      "what was it i don't remember\n",
      "what was it i don ' t remember\n",
      "in jack o'donnell's book trumped he talked about 's weird sleep habits.\n",
      "in jack o ' donnell ' s book trumped he talked about ' s weird sleep habits .\n",
      "i need her in my life\n",
      "i need her in my life\n",
      "honestly, truly, i have never identified more with a picture of a puppy in my entire life\n",
      "honestly , truly , i have never identified more with a picture of a puppy in my entire life\n",
      "counterpoint: it got worse\n",
      "counterpoint : it got worse\n",
      "the back one\n",
      "the back one\n",
      "actually, yes. i am creating the flyer as we speak. will post the info soon. thank you.\n",
      "actually , yes . i am creating the flyer as we speak . will post the info soon . thank you .\n",
      "i. cant. even.\n",
      "i . cant . even .\n",
      "they’re just gonna keep rolling in, son :)\n",
      "they ’ re just gonna keep rolling in , son :)\n",
      "thanks i forgot the name!! i woulda just dm'd you bit i didn't think you'd be up &lt;3\n",
      "thanks i forgot the name !! i woulda just dm ' d you bit i didn ' t think you ' d be up & lt ; 3\n",
      "this is pretty fascinating\n",
      "this is pretty fascinating\n",
      "now, somebody go tell gary where chicago is\n",
      "now , somebody go tell gary where chicago is\n",
      "i already have our colors pick :))))\n",
      "i already have our colors pick :))))\n",
      "britt in was dying at work. i had to close my office door cause i was laughing so hard 😂😂😂\n",
      "britt in was dying at work . i had to close my office door cause i was laughing so hard 😂😂😂\n",
      "here's an example\n",
      "here ' s an example\n",
      "latest text from says \"media will come after us\" if they don't meet their fundraising goal\n",
      "latest text from says \" media will come after us \" if they don ' t meet their fundraising goal\n",
      "this was his last chance and he failed another drug test. he will be suspended indefinitely. not just my opinion\n",
      "this was his last chance and he failed another drug test . he will be suspended indefinitely . not just my opinion\n",
      "absolutely well said. the scariest thing is to see majority of the people loosing their rationality.\n",
      "absolutely well said . the scariest thing is to see majority of the people loosing their rationality .\n",
      "*thus revealing in my mind, mahou is the fallout boy scholar*\n",
      "* thus revealing in my mind , mahou is the fallout boy scholar *\n",
      "no worries. it's twitter.\n",
      "no worries . it ' s twitter .\n",
      "oh no when did he die? somehow i totally missed that. that's awful :(\n",
      "oh no when did he die ? somehow i totally missed that . that ' s awful :(\n",
      "in a restaurant, sure. unless they were gorilla glass. i'm surprised that the iphone doesn't suck.\n",
      "in a restaurant , sure . unless they were gorilla glass . i ' m surprised that the iphone doesn ' t suck .\n",
      "\"shout out to the graphics team\" 😂\n",
      "\" shout out to the graphics team \" 😂\n",
      "i have watched since dec 2013 as every side has attacked one flank.. the brana flank for goal after goal, never dave's side\n",
      "i have watched since dec 2013 as every side has attacked one flank .. the brana flank for goal after goal , never dave ' s side\n",
      "got some fire for you on sunday too\n",
      "got some fire for you on sunday too\n",
      "especially with books, it's a nightmare. and nyc homes are not all that conducive to it...\n",
      "especially with books , it ' s a nightmare . and nyc homes are not all that conducive to it ...\n",
      "if you are attacked maliciously, don't you want to defend yourself? if not, you're crazy.\n",
      "if you are attacked maliciously , don ' t you want to defend yourself ? if not , you ' re crazy .\n",
      "sweet baby, here is raining,but anyway sunshine in our heart 💚☔🌞💛\n",
      "sweet baby , here is raining , but anyway sunshine in our heart 💚☔🌞💛\n",
      "no not rape. check again.\n",
      "no not rape . check again .\n",
      "one significant reason: because they don't have their own money or connections to rich donors to mount a campaign.\n",
      "one significant reason : because they don ' t have their own money or connections to rich donors to mount a campaign .\n",
      "i dont have sources i am just a guy with a phone who reads the observer. you came to the wrong place if you want links\n",
      "i dont have sources i am just a guy with a phone who reads the observer . you came to the wrong place if you want links\n",
      "and how does that usually go over for you?\n",
      "and how does that usually go over for you ?\n",
      "true. it has been one bumpy bus ride.\n",
      "true . it has been one bumpy bus ride .\n",
      ", please, please!, confirm that you will honor this promise to us:\n",
      ", please , please !, confirm that you will honor this promise to us :\n",
      "thats right the secret is out! has now! classic sitcoms all free2watch! …\n",
      "thats right the secret is out ! has now ! classic sitcoms all free2watch ! …\n",
      ". a presidential candidate can't cherry pick which rights in bill of rights he supports, i.e. the 2nd, not the 5th, per oath of office\n",
      ". a presidential candidate can ' t cherry pick which rights in bill of rights he supports , i . e . the 2nd , not the 5th , per oath of office\n",
      "he probably thinks its a suburb of syria\n",
      "he probably thinks its a suburb of syria\n",
      "qana in 96 was hezbollah firing 2 missiles behind civilians at israel and israel firing back. lesson is…\n",
      "qana in 96 was hezbollah firing 2 missiles behind civilians at israel and israel firing back . lesson is …\n",
      "dream knight\n",
      "dream knight\n",
      "hard to want something that completely contradicts reality.\n",
      "hard to want something that completely contradicts reality .\n",
      "i recommend best source in my book\n",
      "i recommend best source in my book\n",
      "tryna keep up w/ the highs on the hook &amp; her runs while riding a bicycle is a workout tho\n",
      "tryna keep up w / the highs on the hook & amp ; her runs while riding a bicycle is a workout tho\n",
      "yes they are very positive they got me through with their music ❤\n",
      "yes they are very positive they got me through with their music ❤\n",
      "not necessary\n",
      "not necessary\n",
      "when u take the r ur really taking the l\n",
      "when u take the r ur really taking the l\n",
      "the best proto-type of a smiling liar is a smiling chimpanzee.\n",
      "the best proto - type of a smiling liar is a smiling chimpanzee .\n",
      "im sad i wasn't asked to participate\n",
      "im sad i wasn ' t asked to participate\n",
      "i think your tests are passing - your build gecko is going from yellow to green 🐉\n",
      "i think your tests are passing - your build gecko is going from yellow to green 🐉\n",
      "why is he telling that old facebook joke? does he know mi jobs increase?\n",
      "why is he telling that old facebook joke ? does he know mi jobs increase ?\n",
      "after saying at he would accept result of election now warning again it could be stolen &amp; nobody wants to talk about it\n",
      "after saying at he would accept result of election now warning again it could be stolen & amp ; nobody wants to talk about it\n",
      "happy birthday kevin!! 🎉\n",
      "happy birthday kevin !! 🎉\n",
      "911 likely voters??? conspiracy!!!!\n",
      "911 likely voters ??? conspiracy !!!!\n",
      "i can vouch for this statement :)\n",
      "i can vouch for this statement :)\n",
      "get the other ones\n",
      "get the other ones\n",
      "no price discovery, manipulation, fugazy behavior. as mohamed el erian famously states..welcome to the new nrml\n",
      "no price discovery , manipulation , fugazy behavior . as mohamed el erian famously states .. welcome to the new nrml\n",
      "he was a better partner for dillon than polak. underrated offensive ability.\n",
      "he was a better partner for dillon than polak . underrated offensive ability .\n",
      "dead inside\n",
      "dead inside\n",
      "sunday, coming back from a rave.could not find my metro card to get on the subway.\n",
      "sunday , coming back from a rave . could not find my metro card to get on the subway .\n",
      "thank you so much for sharing my migraine story. we should talk! would it be possible to email you directly?\n",
      "thank you so much for sharing my migraine story . we should talk ! would it be possible to email you directly ?\n",
      "just caffeine and kilos now? 😜\n",
      "just caffeine and kilos now ? 😜\n",
      "she then still had no right to shoot\n",
      "she then still had no right to shoot\n",
      "lol thank you !\n",
      "lol thank you !\n",
      "it was great!(i watched just two)\n",
      "it was great !( i watched just two )\n",
      "😂😂it's lyrics\n",
      "😂😂 it ' s lyrics\n",
      "matey mate decisions by your \"manager\" have been piss poor then again i am australian so do not care\n",
      "matey mate decisions by your \" manager \" have been piss poor then again i am australian so do not care\n",
      "like so so so seriously! i love it 😆\n",
      "like so so so seriously ! i love it 😆\n",
      "hmmm sounds like a battle i will have to judge\n",
      "hmmm sounds like a battle i will have to judge\n",
      "i been getting smiles &amp; waves tho\n",
      "i been getting smiles & amp ; waves tho\n",
      "just go back to kenya with your muzzie friends\n",
      "just go back to kenya with your muzzie friends\n",
      "yeah dude i would definitely consider a daniel defence super reliable and they are just bad ass\n",
      "yeah dude i would definitely consider a daniel defence super reliable and they are just bad ass\n",
      "i'm dead not looking forward to this\n",
      "i ' m dead not looking forward to this\n",
      "or just insert itl to make .\n",
      "or just insert itl to make .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "tokenizer =  WordPunctTokenizer()\n",
    "def splitSentence(paragraph):  \n",
    "    \n",
    "    sentences = tokenizer.tokenize(paragraph)  \n",
    "    return sentences  \n",
    "\n",
    "last_tokens = set([',','.','?','!',\"\"])\n",
    "def filter(text):\n",
    "#     tokens = text.split(' ')\n",
    "#     res = []\n",
    "#     for token in tokens:\n",
    "#         last_char = token[-1]\n",
    "#         if last_char in last_tokens and len(token) > 1:\n",
    "#             res.append(token[0:-1])\n",
    "#             res.append(last_char)\n",
    "#         else:\n",
    "#             res.append(token)\n",
    "    return ' '.join(splitSentence(text))\n",
    "\n",
    "\n",
    "for line in responses[0:100]:\n",
    "    print(line)\n",
    "    print(filter(line)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 1296159\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "new_messages = []\n",
    "new_responses = []\n",
    "msg_counter = defaultdict(int)\n",
    "res_counter = defaultdict(int)\n",
    "str_counter = defaultdict(int)\n",
    "for msg,res in zip(messages,responses):\n",
    "    if msg == res:\n",
    "        continue\n",
    "    fmsg = filter(msg)\n",
    "    fres = filter(res)\n",
    "    vt = fmsg +'alink' + fres\n",
    "    if msg_counter[fmsg] > 100 or res_counter[fres] > 100 or str_counter[vt] > 50 :\n",
    "        continue\n",
    "    if fmsg == fres:\n",
    "        continue\n",
    "    msg_counter[fmsg] += 1\n",
    "    res_counter[fres]  += 1\n",
    "    str_counter[vt] += 1\n",
    "    \n",
    "    fmsg = fmsg.split()\n",
    "    fres = fres.split()\n",
    "    if len(fmsg) < 2 or len(fres) < 2 or len(fmsg) > 20 or len(fres) > 20:\n",
    "        continue\n",
    "    new_messages.append(fmsg)\n",
    "    new_responses.append(fres)\n",
    "print('total lines: %d' % (len(new_messages))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.00\n",
      "3 0.00\n",
      "4 0.01\n",
      "5 0.03\n",
      "6 0.09\n",
      "7 0.17\n",
      "8 0.27\n",
      "9 0.37\n",
      "10 0.46\n",
      "11 0.54\n",
      "12 0.62\n",
      "13 0.69\n",
      "14 0.76\n",
      "15 0.81\n",
      "16 0.87\n",
      "17 0.91\n",
      "18 0.95\n",
      "19 0.98\n",
      "20 1.00\n",
      "char_nums_per_word:\n",
      "1 0.58\n",
      "2 0.94\n",
      "3 0.98\n",
      "4 0.99\n",
      "5 1.00\n",
      "6 1.00\n",
      "7 1.00\n",
      "8 1.00\n",
      "9 1.00\n",
      "10 1.00\n",
      "11 1.00\n",
      "12 1.00\n",
      "13 1.00\n",
      "14 1.00\n",
      "15 1.00\n",
      "16 1.00\n",
      "17 1.00\n",
      "18 1.00\n",
      "19 1.00\n",
      "20 1.00\n",
      "21 1.00\n",
      "22 1.00\n",
      "23 1.00\n",
      "24 1.00\n",
      "25 1.00\n",
      "26 1.00\n",
      "27 1.00\n",
      "28 1.00\n",
      "29 1.00\n",
      "30 1.00\n",
      "31 1.00\n",
      "32 1.00\n",
      "33 1.00\n",
      "34 1.00\n",
      "35 1.00\n",
      "36 1.00\n",
      "37 1.00\n",
      "38 1.00\n",
      "39 1.00\n",
      "40 1.00\n",
      "41 1.00\n",
      "42 1.00\n",
      "43 1.00\n",
      "44 1.00\n",
      "45 1.00\n",
      "46 1.00\n",
      "47 1.00\n",
      "48 1.00\n",
      "49 1.00\n",
      "50 1.00\n",
      "51 1.00\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "56 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "68 1.00\n",
      "78 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = new_messages\n",
    "tokenized_responses = new_responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "import random\n",
    "random.seed(6666)\n",
    "\n",
    "total_num = len(messages)\n",
    "dev_num =  20000\n",
    "test_num = 20000\n",
    "train_num = total_num - dev_num - test_num\n",
    "\n",
    "random_orders = range(0, total_num)\n",
    "messages = tokenized_messages\n",
    "responses =  tokenized_responses\n",
    "file_map = {'message':tokenized_messages, 'response':tokenized_responses}\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_nums = [20000,40000,60000,80000]\n",
    "for vocab_num in vocab_nums:\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120929\n",
      "0 0.0556\n",
      "10000 0.8991\n",
      "20000 0.9420\n",
      "30000 0.9620\n",
      "40000 0.9752\n",
      "50000 0.9839\n",
      "60000 0.9896\n",
      "70000 0.9934\n",
      "80000 0.9960\n",
      "90000 0.9977\n",
      "100000 0.9989\n",
      "110000 0.9996\n",
      "120000 1.0000\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
