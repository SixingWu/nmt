{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换成字符层级的 带有空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "language = 'chinese'\n",
    "#language = 'english'\n",
    "print(language)\n",
    "if language == 'chinese':\n",
    "    workpath = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/charspace/'\n",
    "    min_fre = 100\n",
    "elif language == 'english':\n",
    "    workpath = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/encharspace/'\n",
    "    min_fre = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['嗷', '嗷', '<SPACE>', '大', '<SPACE>', '湿', '的', '<SPACE>', '左', '手', '<SPACE>', '在', '<SPACE>', '干', '嘛', '<SPACE>', '，', '<SPACE>', '看', '<SPACE>', '着', '<SPACE>', '小', '<SPACE>', '纯', '洁', '<SPACE>', '撸', '么', '<SPACE>', '。']\n"
     ]
    }
   ],
   "source": [
    "def sentece2char_withspace(sentence):\n",
    "    words = sentence.split()\n",
    "    chars = []\n",
    "    for i,word in enumerate(words):\n",
    "        for char in word:\n",
    "            chars.append(char)\n",
    "        if i < len(words) - 1:\n",
    "            chars.append('<SPACE>')\n",
    "    return chars\n",
    "print(sentece2char_withspace('嗷嗷 大 湿的 左手 在 干嘛 ， 看 着 小 纯洁 撸么 。'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3788600\n"
     ]
    }
   ],
   "source": [
    "#要保证顺序对，这样可以不用再次读入\n",
    "set_types = ['dev','test','train']\n",
    "pair_types = ['response','message']\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "\n",
    "for st in set_types:\n",
    "    for pt in pair_types:\n",
    "        if pt == 'response':\n",
    "            container = responses\n",
    "        else:\n",
    "            container = messages\n",
    "        with open(\"%s%s.%s\" % (workpath,st,pt),'r',encoding='utf-8') as fin:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                container.append(sentece2char_withspace(line.strip('\\n')))\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per sentence:\n",
      "3 0.00\n",
      "4 0.00\n",
      "5 0.00\n",
      "6 0.00\n",
      "7 0.00\n",
      "8 0.00\n",
      "9 0.00\n",
      "10 0.00\n",
      "11 0.00\n",
      "12 0.01\n",
      "13 0.02\n",
      "14 0.05\n",
      "15 0.08\n",
      "16 0.12\n",
      "17 0.16\n",
      "18 0.20\n",
      "19 0.24\n",
      "20 0.28\n",
      "21 0.32\n",
      "22 0.36\n",
      "23 0.40\n",
      "24 0.43\n",
      "25 0.47\n",
      "26 0.50\n",
      "27 0.53\n",
      "28 0.57\n",
      "29 0.60\n",
      "30 0.63\n",
      "31 0.66\n",
      "32 0.68\n",
      "33 0.71\n",
      "34 0.74\n",
      "35 0.76\n",
      "36 0.79\n",
      "37 0.81\n",
      "38 0.83\n",
      "39 0.85\n",
      "40 0.87\n",
      "41 0.89\n",
      "42 0.91\n",
      "43 0.93\n",
      "44 0.95\n",
      "45 0.96\n",
      "46 0.97\n",
      "47 0.98\n",
      "48 0.99\n",
      "49 0.99\n",
      "50 0.99\n",
      "51 0.99\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "60 1.00\n",
      "61 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "67 1.00\n",
      "68 1.00\n",
      "69 1.00\n",
      "70 1.00\n",
      "71 1.00\n",
      "72 1.00\n",
      "73 1.00\n",
      "74 1.00\n",
      "75 1.00\n",
      "76 1.00\n",
      "77 1.00\n",
      "78 1.00\n",
      "79 1.00\n",
      "80 1.00\n",
      "81 1.00\n",
      "82 1.00\n",
      "83 1.00\n",
      "84 1.00\n",
      "85 1.00\n",
      "86 1.00\n",
      "87 1.00\n",
      "88 1.00\n",
      "89 1.00\n",
      "90 1.00\n",
      "91 1.00\n",
      "92 1.00\n",
      "93 1.00\n",
      "char_nums_per_word:\n",
      "1 0.62\n",
      "7 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = messages\n",
    "tokenized_responses = responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "print('tokens per sentence:')\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_map = {'message':messages, 'response':responses}\n",
    "dev_num = 20000\n",
    "test_num = 20000\n",
    "total_num = len(messages)\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_path = out_path\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6689\n",
      "4328\n",
      "10550\n",
      "4082\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_message_counter))\n",
    "filtered_sorted_message_counter = list()\n",
    "for item in sorted_message_counter:\n",
    "    if (item[1] >= min_fre):\n",
    "        filtered_sorted_message_counter.append(item)\n",
    "print(len(filtered_sorted_message_counter))\n",
    "\n",
    "print(len(sorted_response_counter))\n",
    "filtered_sorted_response_counter = list()\n",
    "for item in sorted_response_counter:\n",
    "    if (item[1] >= min_fre):\n",
    "        filtered_sorted_response_counter.append(item)\n",
    "print(len(filtered_sorted_response_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab_nums = [40000]\n",
    "for vocab_num in vocab_nums:\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in filtered_sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(filtered_sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6689\n",
      "0 0.3768\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n",
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "seg_len = 5\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [20000,40000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 999\n",
    "            vocab[seg_pad] = 999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 20:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        if file_path[-len('message'):] == 'message':\n",
    "            nfile_path = file_path[0:-len('message')]+''+str(vocab)+'.message'\n",
    "        else:\n",
    "            nfile_path = file_path[0:-len('response')]+''+str(vocab)+'.response'\n",
    "        with open(nfile_path , 'w+', encoding='utf-8') as foout,open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                foout.write(line)\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.message' % vocab)\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.response'% vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.response', seg_len,vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
