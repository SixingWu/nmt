{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换成字符层级的 带有空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 4435959\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "in_path = '/ldev/tensorflow/seq2seq/rawdata/DiaTest/'\n",
    "\n",
    "message_path = in_path + 'huaweiFull.message'\n",
    "response_path = in_path + 'huaweiFull.response'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(message_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    messages = [line.strip('\\n') for line in lines]\n",
    "    for line in messages:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "with open(response_path,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    responses = [line.strip('\\n') for line in lines]\n",
    "    for line in responses:\n",
    "        len_counter[len(line)] += 1\n",
    "\n",
    "assert len(messages) == len(responses)\n",
    "\n",
    "print('total lines: %d' % (len(messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['看到 榜样 的 力量 ， 大家 心里 会 更 安心',\n",
       " '某 前锋 完全 没有 预判 跑位 ， 速度 还 没有 对手 后卫 转身 加 回 追 速度快',\n",
       " '它 这 是 间接 暗示 你 ， 不如 搂 在一起 睡 会儿 吧',\n",
       " 'I want to go to Yunnan province in 2013 .',\n",
       " '如果 雍正 那会儿 有 QQ ， 那 《 甄嬛传 》 里边 的 人 … …',\n",
       " '这个 是 日本 的 那个 什么 组合么',\n",
       " '位置 是 创意 的 根源 ， 二师兄 ， 哈哈 。',\n",
       " '我 爱上 了 一 匹 汗 血 宝马 ， 可是 我 没有 马场 也 没有 钱 。',\n",
       " '这个 。 掉 节操 的 问句 ， 床上 咋办 。',\n",
       " '要么 带 套 ， 要么 大 肚 。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "于 老师 不 给 劝劝 架么 告诉 他们 再 挣 也 不 是 老大\n",
      "于 老师 不 给 劝劝 架么 告诉 他们 再 挣 也 不 是 老大#\n",
      "真不愧是 这么 走 出来 的 爹·······\n",
      "真不愧是 这么 走 出来 的 爹·······#\n",
      "嗷嗷 大 湿的 左手 在 干嘛 ， 看 着 小 纯洁 撸么 。\n",
      "嗷嗷 大 湿的 左手 在 干嘛 ， 看 着 小 纯洁 撸么 。#\n",
      "我 已经 快 感动 得 哭了 。\n",
      "我 已经 快 感动 得 哭了 。#\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter(text):\n",
    "    res = re.sub(r'#[\\S ]*#',\"\", text)\n",
    "    res = re.sub(r'（[\\S ]+）',\"\", text)\n",
    "    res = re.sub(r'alink',\"\", res)\n",
    "    res = re.sub(r'[ ]+',\" \", res)\n",
    "    return res.strip()\n",
    "\n",
    "\n",
    "for line in responses[1:5]:\n",
    "    print(line)\n",
    "    print(filter(line) + '#') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['嗷', '嗷', '<SPACE>', '大', '<SPACE>', '湿', '的', '<SPACE>', '左', '手', '<SPACE>', '在', '<SPACE>', '干', '嘛', '<SPACE>', '，', '<SPACE>', '看', '<SPACE>', '着', '<SPACE>', '小', '<SPACE>', '纯', '洁', '<SPACE>', '撸', '么', '<SPACE>', '。']\n"
     ]
    }
   ],
   "source": [
    "def sentece2char_withspace(sentence):\n",
    "    words = sentence.split()\n",
    "    chars = []\n",
    "    for i,word in enumerate(words):\n",
    "        for char in word:\n",
    "            chars.append(char)\n",
    "        if i < len(words) - 1:\n",
    "            chars.append('<SPACE>')\n",
    "    return chars\n",
    "print(sentece2char_withspace('嗷嗷 大 湿的 左手 在 干嘛 ， 看 着 小 纯洁 撸么 。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3788600\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "new_messages = []\n",
    "new_responses = []\n",
    "msg_counter = defaultdict(int)\n",
    "res_counter = defaultdict(int)\n",
    "str_counter = defaultdict(int)\n",
    "for msg,res in zip(messages,responses):\n",
    "    if msg == res:\n",
    "        continue\n",
    "    fmsg = filter(msg)\n",
    "    fres = filter(res)\n",
    "    vt = fmsg +'alink' + fres\n",
    "    if msg_counter[fmsg] > 75 or res_counter[fres] > 75 or str_counter[vt] > 20 :\n",
    "        continue\n",
    "    if fmsg == fres:\n",
    "        continue\n",
    "    msg_counter[fmsg] += 1\n",
    "    res_counter[fres]  += 1\n",
    "    str_counter[vt] += 1\n",
    "    \n",
    "    '''\n",
    "    过滤完成，开始分割\n",
    "    '''\n",
    "    tmsg = fmsg.split()\n",
    "    tres = fres.split()\n",
    "    if len(tmsg) < 2 or len(tres) < 2 or len(tmsg) > 20 or len(tres) > 20:\n",
    "        continue\n",
    "    new_messages.append(sentece2char_withspace(fmsg))\n",
    "    new_responses.append(sentece2char_withspace(fres))\n",
    "print('total lines: %d' % (len(new_messages))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per sentence:\n",
      "3 0.00\n",
      "4 0.00\n",
      "5 0.00\n",
      "6 0.00\n",
      "7 0.00\n",
      "8 0.00\n",
      "9 0.00\n",
      "10 0.00\n",
      "11 0.00\n",
      "12 0.01\n",
      "13 0.02\n",
      "14 0.05\n",
      "15 0.08\n",
      "16 0.12\n",
      "17 0.16\n",
      "18 0.20\n",
      "19 0.24\n",
      "20 0.28\n",
      "21 0.32\n",
      "22 0.36\n",
      "23 0.40\n",
      "24 0.43\n",
      "25 0.47\n",
      "26 0.50\n",
      "27 0.53\n",
      "28 0.57\n",
      "29 0.60\n",
      "30 0.63\n",
      "31 0.66\n",
      "32 0.68\n",
      "33 0.71\n",
      "34 0.74\n",
      "35 0.76\n",
      "36 0.79\n",
      "37 0.81\n",
      "38 0.83\n",
      "39 0.85\n",
      "40 0.87\n",
      "41 0.89\n",
      "42 0.91\n",
      "43 0.93\n",
      "44 0.95\n",
      "45 0.96\n",
      "46 0.97\n",
      "47 0.98\n",
      "48 0.99\n",
      "49 0.99\n",
      "50 0.99\n",
      "51 0.99\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "60 1.00\n",
      "61 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "67 1.00\n",
      "68 1.00\n",
      "69 1.00\n",
      "70 1.00\n",
      "71 1.00\n",
      "72 1.00\n",
      "73 1.00\n",
      "74 1.00\n",
      "75 1.00\n",
      "76 1.00\n",
      "77 1.00\n",
      "78 1.00\n",
      "79 1.00\n",
      "80 1.00\n",
      "81 1.00\n",
      "82 1.00\n",
      "83 1.00\n",
      "84 1.00\n",
      "85 1.00\n",
      "86 1.00\n",
      "87 1.00\n",
      "88 1.00\n",
      "89 1.00\n",
      "90 1.00\n",
      "91 1.00\n",
      "92 1.00\n",
      "93 1.00\n",
      "char_nums_per_word:\n",
      "1 0.62\n",
      "7 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = new_messages\n",
    "tokenized_responses = new_responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "print('tokens per sentence:')\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/charspace/'\n",
    "import random\n",
    "random.seed(6666)\n",
    "\n",
    "total_num = len(messages)\n",
    "dev_num =  20000\n",
    "test_num = 20000\n",
    "train_num = total_num - dev_num - test_num\n",
    "\n",
    "random_orders = range(0, total_num)\n",
    "messages = tokenized_messages\n",
    "responses =  tokenized_responses\n",
    "file_map = {'message':tokenized_messages, 'response':tokenized_responses}\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/charspace/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/charspace/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_nums = [20000,40000,60000,80000]\n",
    "for vocab_num in vocab_nums:\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120929\n",
      "0 0.0556\n",
      "10000 0.8991\n",
      "20000 0.9420\n",
      "30000 0.9620\n",
      "40000 0.9752\n",
      "50000 0.9839\n",
      "60000 0.9896\n",
      "70000 0.9934\n",
      "80000 0.9960\n",
      "90000 0.9977\n",
      "100000 0.9989\n",
      "110000 0.9996\n",
      "120000 1.0000\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n",
      "original num: 3748600\n",
      "new num: 6690\n",
      "original num: 5593\n",
      "new num: 5593\n",
      "original num: 3748600\n",
      "new num: 10551\n",
      "original num: 5572\n",
      "new num: 5572\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "seg_len = 5\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [20000,40000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 999\n",
    "            vocab[seg_pad] = 999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 20:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        if file_path[-len('message'):] == 'message':\n",
    "            nfile_path = file_path[0:-len('message')]+''+str(vocab)+'.message'\n",
    "        else:\n",
    "            nfile_path = file_path[0:-len('response')]+''+str(vocab)+'.response'\n",
    "        with open(nfile_path , 'w+', encoding='utf-8') as foout,open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                foout.write(line)\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.message' % vocab)\n",
    "    add_chars_to_vocab(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/vocab.%d.separate.response'% vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/train.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/test.response', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.message', seg_len,vocab)\n",
    "    convert_to_seg_file(r'/ldev/tensorflow/nmt2/nmt/data/wordlevel/dev.response', seg_len,vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
