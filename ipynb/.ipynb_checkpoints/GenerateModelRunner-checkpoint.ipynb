{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 自动配置参数，帮助自动生成运行文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_hparams(model_dir):\n",
    "  \"\"\"\n",
    "  Load hparams from an existing model directory.\n",
    "  \"\"\"\n",
    "  hparams_file = os.path.join(model_dir, \"hparams\")\n",
    "  if tf.gfile.Exists(hparams_file):\n",
    "    print_out(\"# Loading hparams from %s\" % hparams_file)\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(hparams_file, \"rb\")) as f:\n",
    "      try:\n",
    "        hparams_values = json.load(f)\n",
    "        hparams = tf.contrib.training.HParams(**hparams_values)\n",
    "      except ValueError:\n",
    "        print_out(\"  can't load hparams file\")\n",
    "        return None\n",
    "    return hparams\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def load_or_update_configs(config_path, default_dict=dict()):\n",
    "    \"\"\"\n",
    "    Load configs from an existing config file\n",
    "    \"\"\"\n",
    "    configs = default_dict\n",
    "    try:\n",
    "        with open(config_path,'r+',encoding='utf-8') as fin:\n",
    "            print(\"Load config file: %s\" % config_path)\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip(\"\\n\").strip(\" \")\n",
    "                items = line.split('=')\n",
    "                if line[0] == \"#\":\n",
    "                    continue\n",
    "                elif len(items) != 2:\n",
    "                    print(\"Bad line: %s\" % line)\n",
    "                else:\n",
    "                    key = items[0].strip()\n",
    "                    value = items[1].strip()\n",
    "                    configs[key] = value\n",
    "                    print(\"%s=%s\" % (key,value))\n",
    "    except Exception:\n",
    "        return None\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level or single_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_word_level_model(data_space,model_id,gpu='0',language='chinese',vocab_prefix='vocab',train_prefix='train',test_prefix='test',dev_prefix='dev',preset_configs = dict()):\n",
    "    config = {\n",
    "        'vocab_prefix': data_space + vocab_prefix,\n",
    "        'train_prefix': data_space + train_prefix,\n",
    "        'test_prefix': data_space + test_prefix,\n",
    "        'dev_prefix': data_space + dev_prefix,\n",
    "        'out_dir' : \"models/\" + model_id ,\n",
    "    }\n",
    "    config = load_or_update_configs('configs/basic.config',config)\n",
    "    if language == 'english':\n",
    "        config = load_or_update_configs('configs/en_wl_offset.config',config)\n",
    "    if language == 'chinese_char':\n",
    "        config = load_or_update_configs('configs/cn_charlevel_offset.config',config)\n",
    "    if language == 'english_char':\n",
    "        config = load_or_update_configs('configs/en_charlevel_offset.config',config)\n",
    "    \n",
    "    # preset\n",
    "    for key in preset_configs.keys():\n",
    "        config[key] = preset_configs[key]\n",
    "        print(\"preset: %s=%s\" % (key,preset_configs[key]))\n",
    "    \n",
    "    with open(\"../%s.sh\" % model_id,'w+',encoding='utf-8') as fin:\n",
    "        out = \"export CUDA_VISIBLE_DEVICES=%s \\n\\n\" % gpu\n",
    "        out += \"python3 -m nrm.nrm  \" \n",
    "        for key in config:\n",
    "            out += \"    --%s=%s  \" % (key, config[key])\n",
    "        out += \"   >> logs/%s.txt \\n\" % model_id\n",
    "        # print(out)\n",
    "        fin.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load config file: configs/basic.config\n",
      "num_units=512\n",
      "embed_dim=340\n",
      "num_layers=4\n",
      "unit_type=gru\n",
      "share_vocab=False\n",
      "src_max_len=20\n",
      "tgt_max_len=20\n",
      "batch_size=256\n",
      "dropout=0.3\n",
      "encoder_type=bi\n",
      "infer_batch_size=10\n",
      "attention=luong\n",
      "src=message\n",
      "tgt=response\n",
      "num_train_steps=1000000\n",
      "steps_per_stats=100\n",
      "metrics=rouge,bleu-1,bleu-2,bleu-3,bleu-4,distinct-1,distinct-2\n",
      "Load config file: configs/en_wl_offset.config\n",
      "src_max_len=30\n",
      "tgt_max_len=30\n",
      "Load config file: configs/basic.config\n",
      "num_units=512\n",
      "embed_dim=340\n",
      "num_layers=4\n",
      "unit_type=gru\n",
      "share_vocab=False\n",
      "src_max_len=20\n",
      "tgt_max_len=20\n",
      "batch_size=256\n",
      "dropout=0.3\n",
      "encoder_type=bi\n",
      "infer_batch_size=10\n",
      "attention=luong\n",
      "src=message\n",
      "tgt=response\n",
      "num_train_steps=1000000\n",
      "steps_per_stats=100\n",
      "metrics=rouge,bleu-1,bleu-2,bleu-3,bleu-4,distinct-1,distinct-2\n",
      "Load config file: configs/basic.config\n",
      "num_units=512\n",
      "embed_dim=340\n",
      "num_layers=4\n",
      "unit_type=gru\n",
      "share_vocab=False\n",
      "src_max_len=20\n",
      "tgt_max_len=20\n",
      "batch_size=256\n",
      "dropout=0.3\n",
      "encoder_type=bi\n",
      "infer_batch_size=10\n",
      "attention=luong\n",
      "src=message\n",
      "tgt=response\n",
      "num_train_steps=1000000\n",
      "steps_per_stats=100\n",
      "metrics=rouge,bleu-1,bleu-2,bleu-3,bleu-4,distinct-1,distinct-2\n",
      "Load config file: configs/cn_charlevel_offset.config\n",
      "src_max_len=50\n",
      "tgt_max_len=50\n",
      "metrics=rouge@space,bleu-1@space,bleu-2@space,bleu-3@space,bleu-4@space,distinct-1@space,distinct-2@space\n",
      "Load config file: configs/basic.config\n",
      "num_units=512\n",
      "embed_dim=340\n",
      "num_layers=4\n",
      "unit_type=gru\n",
      "share_vocab=False\n",
      "src_max_len=20\n",
      "tgt_max_len=20\n",
      "batch_size=256\n",
      "dropout=0.3\n",
      "encoder_type=bi\n",
      "infer_batch_size=10\n",
      "attention=luong\n",
      "src=message\n",
      "tgt=response\n",
      "num_train_steps=1000000\n",
      "steps_per_stats=100\n",
      "metrics=rouge,bleu-1,bleu-2,bleu-3,bleu-4,distinct-1,distinct-2\n",
      "Load config file: configs/en_charlevel_offset.config\n",
      "src_max_len=140\n",
      "tgt_max_len=140\n",
      "metrics=rouge@space,bleu-1@space,bleu-2@space,bleu-3@space,bleu-4@space,distinct-1@space,distinct-2@space\n"
     ]
    }
   ],
   "source": [
    "generate_word_level_model('/ldev/tensorflow/nmt2/nmt/data/enwordlevel/','enword_gru',language='english',vocab_prefix=\"vocab.40000.separate\")\n",
    "generate_word_level_model('/ldev/tensorflow/nmt2/nmt/data/wordlevel/','chinese_gru',vocab_prefix=\"vocab.40000.separate\")\n",
    "generate_word_level_model('/ldev/tensorflow/nmt2/nmt/data/charspace/','chinese_char_gru',language='chinese_char',vocab_prefix=\"vocab.40000.separate\")\n",
    "generate_word_level_model('/ldev/tensorflow/nmt2/nmt/data/encharspace/','english_char_gru',language='english_char',vocab_prefix=\"vocab.40000.separate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
