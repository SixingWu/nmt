{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取所有Chracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6688\n",
      "120929\n",
      "10549\n",
      "618079\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "\n",
    "words_dict = {}\n",
    "chars_dict = {}\n",
    "for file in ['message','response']:\n",
    "    characters = defaultdict(int)\n",
    "    words = defaultdict(int)\n",
    "    with open(in_path+'train.'+file,'r',encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            tokens = line.strip('\\n').split()\n",
    "            for token in tokens:\n",
    "                words[token] += 1\n",
    "                for char in token:\n",
    "                    characters[char] += 1\n",
    "        print(len(characters))\n",
    "        print(len(words))\n",
    "        words_dict[file] = words\n",
    "        chars_dict[file] = characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写一个程序，控制Character的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "def count(total_count,file,output_id):\n",
    "    words = words_dict[file]\n",
    "    chars = (chars_dict[file].keys())\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    for i in range(0,total_count - len(chars)):\n",
    "        vocab.add(sorted_words[i][0])\n",
    "    for i in range(total_count - len(chars)):\n",
    "        for char in sorted_words[i][0]:\n",
    "            vocab.add(char)\n",
    "    with open('/ldev/tensorflow/nmt2/nmt/data/hybridlevel/vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(list(vocab)))\n",
    "    print(len(vocab))\n",
    "\n",
    "# count(44616,'message',40000)\n",
    "# count(48800,'response',40000)\n",
    "# count(25206,'message',20000)\n",
    "# count(29218,'response',20000)\n",
    "# count(83918,'message',80000)\n",
    "# count(88539,'response',80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取对应的文件，然后生成相应的标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_per_sentence: message 20000\n",
      "2 0.003 0.004 0.005 0.016 0.027 0.058 0.099 0.1610 0.2311 0.3112 0.3913 0.4714 0.5515 0.6316 0.7017 0.7718 0.8319 0.8820 0.9321 0.9522 0.9723 0.9824 0.9825 0.9926 0.9927 0.9928 0.9929 0.9930 1.0031 1.0032 1.0033 1.0034 1.0035 1.0036 1.0037 1.0038 1.0039 1.0040 1.0041 1.0042 1.0043 1.0044 1.0045 1.0046 1.0047 1.0048 1.0049 1.0050 1.0051 1.0052 1.0053 1.0054 1.0056 1.0057 1.00\n",
      "items_per_sentence: message 40000\n",
      "2 0.003 0.004 0.005 0.016 0.027 0.068 0.129 0.1910 0.2711 0.3512 0.4413 0.5214 0.6015 0.6816 0.7517 0.8218 0.8819 0.9320 0.9621 0.9822 0.9823 0.9924 0.9925 0.9926 0.9927 1.0028 1.0029 1.0030 1.0031 1.0032 1.0033 1.0034 1.0035 1.0036 1.0037 1.0038 1.0039 1.0040 1.0041 1.0042 1.0043 1.0044 1.0045 1.0046 1.0047 1.0048 1.0049 1.0050 1.0051 1.0052 1.0055 1.0057 1.00\n",
      "items_per_sentence: message 80000\n",
      "2 0.003 0.004 0.005 0.016 0.037 0.078 0.149 0.2110 0.3011 0.3912 0.4813 0.5614 0.6515 0.7316 0.8017 0.8618 0.9219 0.9620 0.9921 1.0022 1.0023 1.0024 1.0025 1.0026 1.0027 1.0028 1.0029 1.0030 1.0031 1.0032 1.0033 1.0034 1.0035 1.0036 1.0037 1.0038 1.0039 1.0040 1.0041 1.0042 1.0043 1.0044 1.0046 1.0047 1.0048 1.0049 1.0050 1.0051 1.0052 1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-5bca26d714f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "for file in ['message','response']:\n",
    "    for vocab_size in [20000,40000,80000]:\n",
    "        vocab = set()\n",
    "        with open('/ldev/tensorflow/nmt2/nmt/data/hybridlevel/vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            with open('/ldev/tensorflow/nmt2/nmt/data/hybridlevel/'+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                            if token not in vocab:\n",
    "                                res = res + list(token)\n",
    "                            else:\n",
    "                                res.append(token)\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "            \n",
    "            if prefix == 'train':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
