{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首先按照HL-EncDec的方式处理Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language = 'en2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3594\n",
      "308257\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "if language == 'cn':\n",
    "    in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/allevel/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "elif language == 'en':\n",
    "    in_path = '/home/mebiuw/nmt/data/enwordlevel/'\n",
    "    in_path = '/home/mebiuw/nmt/data/enallevel/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "elif language =='en2':\n",
    "    in_path = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/enallevel/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "\n",
    "words_dict = {}\n",
    "chars_dict = {}\n",
    "for file in ['message']:\n",
    "    characters = defaultdict(int)\n",
    "    words = defaultdict(int)\n",
    "    with open(in_path+'train.'+file,'r',encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            tokens = line.strip('\\n').split()\n",
    "            for token in tokens:\n",
    "                words[token] += 1\n",
    "                for char in token:\n",
    "                    characters[char] += 1\n",
    "        print(len(characters))\n",
    "        print(len(words))\n",
    "        words_dict[file] = words\n",
    "        chars_dict[file] = characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写一个程序，控制Character的数量\n",
    "\n",
    "AL-HLEncDec 保证全量的词表大小\n",
    "\n",
    "Character 包含双份的，即包含未终结的字符\n",
    "\n",
    "Response : Response部分，Decoder部分，字和词直接混合在一起\n",
    "Message： Post部分字和词需要分开\n",
    "\n",
    "K = 4 for Coling HL-EncDec,K  = 1 For EMNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message char nums: 539\n",
      "message : 40000\n"
     ]
    }
   ],
   "source": [
    "if language == 'cn':\n",
    "    min_fre = 120\n",
    "else:\n",
    "    min_fre = 80\n",
    "\n",
    "\n",
    "\n",
    "# K = 4 for Coling HL-EncDec,K  = 1 For EMNLP\n",
    "def count_message(total_count,file,output_id,k=4):\n",
    "    words = words_dict[file]\n",
    "    chars = []\n",
    "    for char in chars_dict[file].keys():\n",
    "        if chars_dict[file][char] >= min_fre:\n",
    "            chars.append(char)\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    print('message char nums: %d' % len(chars) )\n",
    "    for i in range(total_count):\n",
    "        vocab.add(sorted_words[i][0])   \n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open(out_path+'vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(list(vocab)))\n",
    "    print(\"message : %d\" % len(vocab))\n",
    "\n",
    "\n",
    "count_message(40000,'message',40000,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取对应的文件，然后生成相应的混合表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2546761\n",
      "88\n",
      "items_per_sentence: message 40000\n",
      "2 0.00 3 0.02 4 0.04 5 0.07 6 0.10 7 0.14 8 0.18 9 0.22 10 0.25 11 0.29 12 0.33 13 0.37 14 0.41 15 0.45 16 0.48 17 0.52 18 0.56 19 0.59 20 0.63 21 0.67 22 0.71 23 0.74 24 0.78 25 0.82 26 0.85 27 0.88 28 0.91 29 0.93 30 0.95 31 0.96 32 0.97 33 0.98 34 0.99 35 0.99 36 0.99 37 1.00 38 1.00 39 1.00 40 1.00 41 1.00 42 1.00 43 1.00 44 1.00 45 1.00 46 1.00 47 1.00 48 1.00 49 1.00 50 1.00 51 1.00 52 1.00 53 1.00 54 1.00 55 1.00 56 1.00 57 1.00 58 1.00 59 1.00 60 1.00 61 1.00 62 1.00 63 1.00 64 1.00 65 1.00 66 1.00 67 1.00 69 1.00 70 1.00 \n",
      "20000\n",
      "51\n",
      "20000\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for file in ['message']:\n",
    "    for vocab_size in [40000]:\n",
    "        vocab = set()\n",
    "        with open(out_path+'vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            max_len = 0\n",
    "            with open(out_path+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                                res.append(token)\n",
    "                        max_len = max(len(res), max_len)\n",
    "                        res = res[0:70]\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "                    print(total_len)\n",
    "            print(max_len)\n",
    "            if prefix == 'train':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复制bpe的response到对应的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpelevel/train.response  /ldev/tensorflow/nmt2/nmt/data/enallevel/train.40000.response\n",
      "0\n",
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpelevel/test.response  /ldev/tensorflow/nmt2/nmt/data/enallevel/test.40000.response\n",
      "0\n",
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpelevel/dev.response  /ldev/tensorflow/nmt2/nmt/data/enallevel/dev.40000.response\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if language == 'cn':\n",
    "    bpe_path = '/ldev/tensorflow/nmt2/nmt/data/bpelevel/'\n",
    "elif language == 'en':\n",
    "    bpe_path = '/home/mebiuw/nmt/data/enbpelevel/'\n",
    "elif language =='en2':\n",
    "    bpe_path = '/ldev/tensorflow/nmt2/nmt/data/enbpelevel/'\n",
    "\n",
    "files = ['train','test','dev']\n",
    "for file in files:\n",
    "    bash = 'cp %s%s.response  %s%s.40000.response' % (bpe_path,file,out_path,file)\n",
    "    print(bash)\n",
    "    print(os.system(bash))\n",
    "print(os.system('cp %svocab.40000.separate.response %svocab.40000.response' % (bpe_path, out_path)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 2546761\n",
      "new num: 3596\n",
      "original num: 3596\n",
      "new num: 3598\n",
      "original num: 2546761\n",
      "new num: 2786\n",
      "original num: 2786\n",
      "new num: 2788\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "if language == 'cn':\n",
    "    seg_len = 5\n",
    "else:\n",
    "    seg_len = 10 # 10已经足够了 99%\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [40000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 99999\n",
    "            vocab[seg_pad] = 99999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "            \n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        nfile_path = file_path\n",
    "        with open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(out_path+r'train.%d.message' % (vocab), out_path+r'vocab.%d.message' % vocab)\n",
    "    add_chars_to_vocab(out_path+r'train.%d.response'% (vocab), out_path+r'vocab.%d.response'% vocab)\n",
    "    convert_to_seg_file(out_path+r'train.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'train.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'test.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'test.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'dev.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'dev.%d.response'% (vocab), seg_len,vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
