{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首先按照HL-EncDec的方式处理Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language = 'en2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435\n",
      "119545\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "if language == 'cn':\n",
    "    in_path = '/ldev/tensorflow/nmt2/nmt/data/wordlevel/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/allevel/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "elif language == 'en':\n",
    "    in_path = '/home/mebiuw/nmt/data/enwordlevel/'\n",
    "    in_path = '/home/mebiuw/nmt/data/enallevel/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "elif language =='en2':\n",
    "    in_path = '/ldev/tensorflow/nmt2/nmt/data/enword_light/'\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/enal_light/'\n",
    "    os.system('mkdir %s' % out_path)\n",
    "\n",
    "words_dict = {}\n",
    "chars_dict = {}\n",
    "for file in ['message']:\n",
    "    characters = defaultdict(int)\n",
    "    words = defaultdict(int)\n",
    "    with open(in_path+'train.'+file,'r',encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            tokens = line.strip('\\n').split()\n",
    "            for token in tokens:\n",
    "                words[token] += 1\n",
    "                for char in token:\n",
    "                    characters[char] += 1\n",
    "        print(len(characters))\n",
    "        print(len(words))\n",
    "        words_dict[file] = words\n",
    "        chars_dict[file] = characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写一个程序，控制Character的数量\n",
    "\n",
    "AL-HLEncDec 保证全量的词表大小\n",
    "\n",
    "Character 包含双份的，即包含未终结的字符\n",
    "\n",
    "Response : Response部分，Decoder部分，字和词直接混合在一起\n",
    "Message： Post部分字和词需要分开\n",
    "\n",
    "K = 4 for Coling HL-EncDec,K  = 1 For EMNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message char nums: 617\n",
      "message : 15000\n"
     ]
    }
   ],
   "source": [
    "if language == 'cn':\n",
    "    min_fre = 120\n",
    "else:\n",
    "    min_fre = 5\n",
    "\n",
    "\n",
    "\n",
    "# K = 4 for Coling HL-EncDec,K  = 1 For EMNLP\n",
    "def count_message(total_count,file,output_id,k=4):\n",
    "    words = words_dict[file]\n",
    "    chars = []\n",
    "    for char in chars_dict[file].keys():\n",
    "        if chars_dict[file][char] >= min_fre:\n",
    "            chars.append(char)\n",
    "    vocab = set()\n",
    "    sorted_words = sorted(words.items(), key = lambda x:x[1], reverse=True)\n",
    "    print('message char nums: %d' % len(chars) )\n",
    "    for i in range(total_count):\n",
    "        vocab.add(sorted_words[i][0])   \n",
    "    if '@@' in vocab or ' ' in vocab:\n",
    "        print('Error')\n",
    "    with open(out_path+'vocab.'+str(output_id)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('\\n'.join(list(vocab)))\n",
    "    print(\"message : %d\" % len(vocab))\n",
    "\n",
    "\n",
    "count_message(15000,'message',15000,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取对应的文件，然后生成相应的混合表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195561\n",
      "20\n",
      "items_per_sentence: message 15000\n",
      "4 0.04 5 0.09 6 0.15 7 0.22 8 0.28 9 0.34 10 0.41 11 0.47 12 0.54 13 0.60 14 0.66 15 0.72 16 0.78 17 0.84 18 0.89 19 0.95 20 1.00 \n",
      "10000\n",
      "20\n",
      "10000\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for file in ['message']:\n",
    "    for vocab_size in [15000]:\n",
    "        vocab = set()\n",
    "        with open(out_path+'vocab.'+str(vocab_size)+'.'+file,'r+',encoding='utf-8') as fin:\n",
    "            for line in fin.readlines():\n",
    "                vocab.add(line.strip())\n",
    "        for prefix  in ['train','test','dev']:\n",
    "            sen_len_counter = defaultdict(int)\n",
    "            max_len = 0\n",
    "            with open(out_path+prefix+'.'+str(vocab_size)+'.'+file,'w',encoding='utf-8') as fout:\n",
    "                with open(in_path+prefix+'.'+file,'r',encoding='utf-8') as fin:\n",
    "                    total_len = 0\n",
    "                    for line in fin.readlines():\n",
    "                        res = []\n",
    "                        for token in line.strip('\\n').split():\n",
    "                                res.append(token)\n",
    "                        max_len = max(len(res), max_len)\n",
    "                        res = res[0:70]\n",
    "                        fout.write(' '.join(res)+'\\n')\n",
    "                        sen_len_counter[len(res)] += 1\n",
    "                        total_len += 1\n",
    "                    print(total_len)\n",
    "            print(max_len)\n",
    "            if prefix == 'train':\n",
    "                print('items_per_sentence: %s %s' % (file,vocab_size))\n",
    "                sorted_length = sorted(sen_len_counter.items(), key = lambda x:x[0])\n",
    "                total_counter = 0\n",
    "                line_str = ''\n",
    "                for lens,counter in sorted_length:\n",
    "                    total_counter += counter\n",
    "                    line_str += '%d %.2f ' % (lens, total_counter/total_len)\n",
    "                print (line_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复制bpe的response到对应的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpe_light/train.response  /ldev/tensorflow/nmt2/nmt/data/enal_light/train.15000.response\n",
      "0\n",
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpe_light/test.response  /ldev/tensorflow/nmt2/nmt/data/enal_light/test.15000.response\n",
      "0\n",
      "cp /ldev/tensorflow/nmt2/nmt/data/enbpe_light/dev.response  /ldev/tensorflow/nmt2/nmt/data/enal_light/dev.15000.response\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if language == 'cn':\n",
    "    bpe_path = '/ldev/tensorflow/nmt2/nmt/data/bpelevel/'\n",
    "elif language == 'en':\n",
    "    bpe_path = '/home/mebiuw/nmt/data/enbpelevel/'\n",
    "elif language =='en2':\n",
    "    bpe_path = '/ldev/tensorflow/nmt2/nmt/data/enbpe_light/'\n",
    "\n",
    "files = ['train','test','dev']\n",
    "for file in files:\n",
    "    bash = 'cp %s%s.response  %s%s.15000.response' % (bpe_path,file,out_path,file)\n",
    "    print(bash)\n",
    "    print(os.system(bash))\n",
    "print(os.system('cp %svocab.15000.separate.response %svocab.15000.response' % (bpe_path, out_path)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num: 195561\n",
      "new num: 1437\n",
      "original num: 1437\n",
      "new num: 1439\n",
      "original num: 195561\n",
      "new num: 1153\n",
      "original num: 1153\n",
      "new num: 1155\n"
     ]
    }
   ],
   "source": [
    "# Configs including a end\n",
    "if language == 'cn':\n",
    "    seg_len = 5\n",
    "else:\n",
    "    seg_len = 8 # 10已经足够了 99%\n",
    "seg_end = '<#>'\n",
    "seg_pad = '<_>'\n",
    "seg_separator = '\\t'\n",
    "seg_inter_separator = ' '\n",
    "vocab_nums = [15000]\n",
    "from collections import defaultdict\n",
    "# read the vocabs and add all characters in the vocab\n",
    "def add_chars_to_vocab(file_path,vocab_path, word_num = -1):\n",
    "    with open(file_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_tmp','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = defaultdict(int)\n",
    "            vocab[seg_end] = 99999\n",
    "            vocab[seg_pad] = 99999\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                line=line.replace(' ','')\n",
    "                line=line.replace('\\t','')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 1:\n",
    "                    for char in line:\n",
    "                        vocab[char] += 1\n",
    "                \n",
    "            for token in vocab.keys():\n",
    "                if vocab[token] > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                #print(token)\n",
    "            print('new num: %d' % len(vocab))\n",
    "            \n",
    "    with open(vocab_path+'_tmp','r+',encoding='utf-8') as fin:\n",
    "        with open(vocab_path+'_seg','w+',encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            print('original num: %d' % len(lines))\n",
    "            vocab = set()\n",
    "            vocab.add(seg_end)\n",
    "            vocab.add(seg_pad)\n",
    "            \n",
    "            # first 3 lines are special tokens\n",
    "            if word_num != -1:\n",
    "                lines = lines[0:0+word_num]\n",
    "            else:\n",
    "                lines = lines[0:]\n",
    "            for line in lines:\n",
    "                line = line.strip('\\n')\n",
    "                #vocab.add(word)\n",
    "                if len(line) > 0:\n",
    "                    for char in line:\n",
    "                        vocab.add(str(char))\n",
    "            count = 0\n",
    "            for token in vocab:\n",
    "                if len(token) > 0:\n",
    "                    fout.write(token+'\\n')\n",
    "                    count += 1\n",
    "                #print(token)\n",
    "            print('new num: %d' % count)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_seg_file(file_path, seg_len,vocab):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as fin:\n",
    "        nfile_path = file_path\n",
    "        with open(nfile_path + '_seg', 'w+', encoding='utf-8') as fout,open(nfile_path + '_seg_len', 'w+', encoding='utf-8') as flout:\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                items = line.strip('\\n').split(' ')\n",
    "                seg_items = []\n",
    "                for item in items:\n",
    "                    item = list(item)\n",
    "                    item = item[0:seg_len - 1]\n",
    "                    item.append(seg_end)\n",
    "                    while len(item) != seg_len:\n",
    "                        item.append(seg_pad)\n",
    "                    seg_items.append(seg_inter_separator.join(item))\n",
    "                flout.write(' '.join([str(min(seg_len,len(x)+1)) for x in items]) + '\\n')\n",
    "                fout.write(seg_separator.join(seg_items) + '\\n')\n",
    "\n",
    "for vocab in vocab_nums:\n",
    "    # TODO 所有\n",
    "    add_chars_to_vocab(out_path+r'train.%d.message' % (vocab), out_path+r'vocab.%d.message' % vocab)\n",
    "    add_chars_to_vocab(out_path+r'train.%d.response'% (vocab), out_path+r'vocab.%d.response'% vocab)\n",
    "    convert_to_seg_file(out_path+r'train.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'train.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'test.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'test.%d.response'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'dev.%d.message'% (vocab), seg_len,vocab)\n",
    "    convert_to_seg_file(out_path+r'dev.%d.response'% (vocab), seg_len,vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
