{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 2601244\n",
      "total lines: 2601244\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "input_file = '/ldev/tensorflow/nmt2/nmt/data/gitdata/chat_corpus/twitter.big'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(input_file,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    all_lines = [line.strip('\\n') for line in lines]\n",
    "    for i in range(0,len(all_lines),2):\n",
    "        messages.append(all_lines[i])\n",
    "        responses.append(all_lines[i+1])\n",
    "print('total lines: %d' % (len(messages)))\n",
    "print('total lines: %d' % (len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanna bet on it? want the over or under?\n",
      "youâ€™re predicting weâ€™ll know the result precisely 28 minutes after polls close? :-)\n"
     ]
    }
   ],
   "source": [
    "print(responses[88687])\n",
    "print(messages[88687])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paranoia has already began...why do they think shane doesn't wanna work with them? because he's not up their butt?\n",
      "the paranoia has already began ... why do they think shane doesn ' t wanna work with them ? because he ' s not up their butt ?\n",
      "white ford boys ðŸ˜¤ðŸ˜¤ðŸ˜‚\n",
      "white ford boys ðŸ˜¤ðŸ˜¤ðŸ˜‚\n",
      "too bad you don't actually go to my school you liar ðŸ˜‚\n",
      "too bad you don ' t actually go to my school you liar ðŸ˜‚\n",
      "before you go to work open each episode up in its own tab\n",
      "before you go to work open each episode up in its own tab\n",
      "would you have sat and waited to see if he had a gun or would you have saved your own life\n",
      "would you have sat and waited to see if he had a gun or would you have saved your own life\n",
      "aww thanks chalupa i know i can trust your taste ðŸ‘Œ\n",
      "aww thanks chalupa i know i can trust your taste ðŸ‘Œ\n",
      "the nats used to be canadian. try again.\n",
      "the nats used to be canadian . try again .\n",
      ", please, please! the confirm that you will honor this promise:\n",
      ", please , please ! the confirm that you will honor this promise :\n",
      "thank you!!\n",
      "thank you !!\n",
      "keeping my fingers crossed that he still has another ed wood in him before he retires.\n",
      "keeping my fingers crossed that he still has another ed wood in him before he retires .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "tokenizer =  WordPunctTokenizer()\n",
    "def splitSentence(paragraph):  \n",
    "    \n",
    "    sentences = tokenizer.tokenize(paragraph)  \n",
    "    return sentences  \n",
    "\n",
    "last_tokens = set([',','.','?','!',\"\"])\n",
    "def filter(text):\n",
    "#     tokens = text.split(' ')\n",
    "#     res = []\n",
    "#     for token in tokens:\n",
    "#         last_char = token[-1]\n",
    "#         if last_char in last_tokens and len(token) > 1:\n",
    "#             res.append(token[0:-1])\n",
    "#             res.append(last_char)\n",
    "#         else:\n",
    "#             res.append(token)\n",
    "    return ' '.join(splitSentence(text))\n",
    "\n",
    "\n",
    "for line in responses[0:10]:\n",
    "    print(line)\n",
    "    print(filter(line)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 2586761\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "new_messages = []\n",
    "new_responses = []\n",
    "msg_counter = defaultdict(int)\n",
    "res_counter = defaultdict(int)\n",
    "str_counter = defaultdict(int)\n",
    "for msg,res in zip(messages,responses):\n",
    "    if msg == res:\n",
    "        continue\n",
    "    fmsg = filter(msg)\n",
    "    fres = filter(res)\n",
    "    vt = fmsg +'alink' + fres\n",
    "    if msg_counter[fmsg] > 75 or res_counter[fres] > 75 or str_counter[vt] > 20 :\n",
    "        continue\n",
    "    if fmsg == fres:\n",
    "        continue\n",
    "    msg_counter[fmsg] += 1\n",
    "    res_counter[fres]  += 1\n",
    "    str_counter[vt] += 1\n",
    "    \n",
    "    fmsg = fmsg.split()\n",
    "    fres = fres.split()\n",
    "    if len(fmsg) < 2 or len(fres) < 2 or len(fmsg) >100 or len(fres) > 100:\n",
    "        continue\n",
    "    new_messages.append(fmsg)\n",
    "    new_responses.append(fres)\n",
    "print('total lines: %d' % (len(new_messages))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.01\n",
      "3 0.03\n",
      "4 0.06\n",
      "5 0.10\n",
      "6 0.14\n",
      "7 0.18\n",
      "8 0.23\n",
      "9 0.27\n",
      "10 0.31\n",
      "11 0.35\n",
      "12 0.39\n",
      "13 0.43\n",
      "14 0.47\n",
      "15 0.51\n",
      "16 0.54\n",
      "17 0.58\n",
      "18 0.61\n",
      "19 0.65\n",
      "20 0.68\n",
      "21 0.72\n",
      "22 0.75\n",
      "23 0.78\n",
      "24 0.81\n",
      "25 0.85\n",
      "26 0.87\n",
      "27 0.90\n",
      "28 0.92\n",
      "29 0.94\n",
      "30 0.96\n",
      "31 0.97\n",
      "32 0.98\n",
      "33 0.99\n",
      "34 0.99\n",
      "35 0.99\n",
      "36 1.00\n",
      "37 1.00\n",
      "38 1.00\n",
      "39 1.00\n",
      "40 1.00\n",
      "41 1.00\n",
      "42 1.00\n",
      "43 1.00\n",
      "44 1.00\n",
      "45 1.00\n",
      "46 1.00\n",
      "47 1.00\n",
      "48 1.00\n",
      "49 1.00\n",
      "50 1.00\n",
      "51 1.00\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "60 1.00\n",
      "61 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "67 1.00\n",
      "69 1.00\n",
      "70 1.00\n",
      "73 1.00\n",
      "76 1.00\n",
      "80 1.00\n",
      "82 1.00\n",
      "88 1.00\n",
      "char_nums_per_word:\n",
      "1 0.22\n",
      "2 0.37\n",
      "3 0.55\n",
      "4 0.71\n",
      "5 0.81\n",
      "6 0.88\n",
      "7 0.93\n",
      "8 0.96\n",
      "9 0.98\n",
      "10 0.99\n",
      "11 0.99\n",
      "12 1.00\n",
      "13 1.00\n",
      "14 1.00\n",
      "15 1.00\n",
      "16 1.00\n",
      "17 1.00\n",
      "18 1.00\n",
      "19 1.00\n",
      "20 1.00\n",
      "21 1.00\n",
      "22 1.00\n",
      "23 1.00\n",
      "24 1.00\n",
      "25 1.00\n",
      "26 1.00\n",
      "27 1.00\n",
      "28 1.00\n",
      "29 1.00\n",
      "30 1.00\n",
      "31 1.00\n",
      "32 1.00\n",
      "33 1.00\n",
      "34 1.00\n",
      "35 1.00\n",
      "36 1.00\n",
      "37 1.00\n",
      "38 1.00\n",
      "39 1.00\n",
      "40 1.00\n",
      "41 1.00\n",
      "42 1.00\n",
      "43 1.00\n",
      "44 1.00\n",
      "45 1.00\n",
      "46 1.00\n",
      "47 1.00\n",
      "48 1.00\n",
      "49 1.00\n",
      "50 1.00\n",
      "51 1.00\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "58 1.00\n",
      "59 1.00\n",
      "60 1.00\n",
      "61 1.00\n",
      "62 1.00\n",
      "63 1.00\n",
      "64 1.00\n",
      "65 1.00\n",
      "66 1.00\n",
      "67 1.00\n",
      "68 1.00\n",
      "69 1.00\n",
      "70 1.00\n",
      "71 1.00\n",
      "72 1.00\n",
      "74 1.00\n",
      "75 1.00\n",
      "78 1.00\n",
      "79 1.00\n",
      "81 1.00\n",
      "82 1.00\n",
      "83 1.00\n",
      "84 1.00\n",
      "86 1.00\n",
      "87 1.00\n",
      "88 1.00\n",
      "89 1.00\n",
      "91 1.00\n",
      "92 1.00\n",
      "97 1.00\n",
      "98 1.00\n",
      "99 1.00\n",
      "101 1.00\n",
      "102 1.00\n",
      "106 1.00\n",
      "107 1.00\n",
      "112 1.00\n",
      "113 1.00\n",
      "117 1.00\n",
      "118 1.00\n",
      "119 1.00\n",
      "120 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics TODO ç»Ÿè®¡é•¿åº¦\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = new_messages\n",
    "tokenized_responses = new_responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2586761\n",
      "2546761\n"
     ]
    }
   ],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "import random\n",
    "random.seed(6666)\n",
    "\n",
    "total_num = len(messages)\n",
    "dev_num =  20000\n",
    "test_num = 20000\n",
    "train_num = total_num - dev_num - test_num\n",
    "print(total_num)\n",
    "print(train_num)\n",
    "\n",
    "random_orders = range(0, total_num)\n",
    "messages = tokenized_messages\n",
    "responses =  tokenized_responses\n",
    "file_map = {'message':tokenized_messages, 'response':tokenized_responses}\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308257\n",
      "292034\n"
     ]
    }
   ],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "print(len(sorted_message_counter))\n",
    "print(len(sorted_response_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_nums = [40000]\n",
    "for vocab_num in vocab_nums:\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/enwordlevel/'\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308257\n",
      "0 0.0385\n",
      "10000 0.9461\n",
      "20000 0.9712\n",
      "30000 0.9807\n",
      "40000 0.9855\n",
      "50000 0.9884\n",
      "60000 0.9904\n",
      "70000 0.9917\n",
      "80000 0.9928\n",
      "90000 0.9936\n",
      "100000 0.9943\n",
      "110000 0.9948\n",
      "120000 0.9953\n",
      "130000 0.9957\n",
      "140000 0.9962\n",
      "150000 0.9964\n",
      "160000 0.9966\n",
      "170000 0.9969\n",
      "180000 0.9971\n",
      "190000 0.9973\n",
      "200000 0.9975\n",
      "210000 0.9978\n",
      "220000 0.9980\n",
      "230000 0.9982\n",
      "240000 0.9984\n",
      "250000 0.9987\n",
      "260000 0.9989\n",
      "270000 0.9991\n",
      "280000 0.9994\n",
      "290000 0.9996\n",
      "300000 0.9998\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%10000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
