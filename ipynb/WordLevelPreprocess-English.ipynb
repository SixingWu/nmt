{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 377265\n",
      "total lines: 377265\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "\n",
    "input_file = '/ldev/tensorflow/nmt2/chat_corpus/twitter_en.txt'\n",
    "\n",
    "messages = []\n",
    "responses = []\n",
    "with open(input_file,'r+',encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    all_lines = [line.strip('\\n') for line in lines]\n",
    "    for i in range(0,len(all_lines),2):\n",
    "        messages.append(all_lines[i])\n",
    "        responses.append(all_lines[i+1])\n",
    "print('total lines: %d' % (len(messages)))\n",
    "print('total lines: %d' % (len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah dude i would definitely consider a daniel defence super reliable and they are just bad ass\n",
      "yeah dude i would definitely consider a daniel defence super reliable and they are just bad ass\n",
      "i'm dead not looking forward to this\n",
      "i'm dead not looking forward to this\n",
      "or just insert itl to make .\n",
      "or just insert itl to make .\n",
      "how do you do this?\n",
      "how do you do this ?\n",
      "yea you right. but we do live in a world where republicans will harass obama about a birth certificate but won't say\n",
      "yea you right.but we do live in a world where republicans will harass obama about a birth certificate but won't say\n",
      "i love green but 3d parties might elect trump like nader elected bush in 2000 with gore there would not have been iraq war no wmd\n",
      "i love green but 3d parties might elect trump like nader elected bush in 2000 with gore there would not have been iraq war no wmd\n",
      "now you are a walking spoiler...\n",
      "now you are a walking spoiler ...\n",
      "i’m waiting for to wake up :p\n",
      "i ’ m waiting for to wake up : p\n",
      "in seriousness, if the next 8.0 happens in my lifetime, i really hope i'm not walking around downtown sf when it hits.\n",
      "in seriousness , if the next 8.0 happens in my lifetime , i really hope i'm not walking around downtown sf when it hits .\n",
      "i think he is an old pupper ian he just wants some pizza tbh\n",
      "i think he is an old pupper ian he just wants some pizza tbh\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "tokenizer =  WordPunctTokenizer()\n",
    "def splitSentence(paragraph):  \n",
    "    \n",
    "    sentences = tokenizer.tokenize(paragraph)  \n",
    "    return sentences  \n",
    "last_tokens = set([',','.','?','!',\"\"])\n",
    "def filter(text):\n",
    "#     tokens = text.split(' ')\n",
    "#     res = []\n",
    "#     for token in tokens:\n",
    "#         last_char = token[-1]\n",
    "#         if last_char in last_tokens and len(token) > 1:\n",
    "#             res.append(token[0:-1])\n",
    "#             res.append(last_char)\n",
    "#         else:\n",
    "#             res.append(token)\n",
    "    tmp =  ' '.join(splitSentence(text))\n",
    "    tmp = tmp.replace(\" ' \",\"'\")\n",
    "    tmp = tmp.replace(\" . \",\".\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "for line in responses[0:10]:\n",
    "    print(line)\n",
    "    print(filter(line)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 215561\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "new_messages = []\n",
    "new_responses = []\n",
    "msg_counter = defaultdict(int)\n",
    "res_counter = defaultdict(int)\n",
    "str_counter = defaultdict(int)\n",
    "for msg,res in zip(messages,responses):\n",
    "    if msg == res:\n",
    "        continue\n",
    "    fmsg = filter(msg)\n",
    "    fres = filter(res)\n",
    "    vt = fmsg +'alink' + fres\n",
    "    if msg_counter[fmsg] > 75 or res_counter[fres] > 75 or str_counter[vt] > 20 :\n",
    "        continue\n",
    "    if fmsg == fres:\n",
    "        continue\n",
    "    msg_counter[fmsg] += 1\n",
    "    res_counter[fres]  += 1\n",
    "    str_counter[vt] += 1\n",
    "    \n",
    "    fmsg = fmsg.split()\n",
    "    fres = fres.split()\n",
    "    if len(fmsg) < 4 or len(fres) < 4 or len(fmsg) >20 or len(fres) > 20:\n",
    "        continue\n",
    "    new_messages.append(fmsg)\n",
    "    new_responses.append(fres)\n",
    "print('total lines: %d' % (len(new_messages))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.06\n",
      "5 0.12\n",
      "6 0.19\n",
      "7 0.26\n",
      "8 0.34\n",
      "9 0.40\n",
      "10 0.47\n",
      "11 0.53\n",
      "12 0.59\n",
      "13 0.65\n",
      "14 0.70\n",
      "15 0.76\n",
      "16 0.81\n",
      "17 0.86\n",
      "18 0.91\n",
      "19 0.96\n",
      "20 1.00\n",
      "char_nums_per_word:\n",
      "1 0.14\n",
      "2 0.29\n",
      "3 0.48\n",
      "4 0.66\n",
      "5 0.77\n",
      "6 0.84\n",
      "7 0.90\n",
      "8 0.94\n",
      "9 0.96\n",
      "10 0.98\n",
      "11 0.99\n",
      "12 0.99\n",
      "13 0.99\n",
      "14 1.00\n",
      "15 1.00\n",
      "16 1.00\n",
      "17 1.00\n",
      "18 1.00\n",
      "19 1.00\n",
      "20 1.00\n",
      "21 1.00\n",
      "22 1.00\n",
      "23 1.00\n",
      "24 1.00\n",
      "25 1.00\n",
      "26 1.00\n",
      "27 1.00\n",
      "28 1.00\n",
      "29 1.00\n",
      "30 1.00\n",
      "31 1.00\n",
      "32 1.00\n",
      "33 1.00\n",
      "34 1.00\n",
      "35 1.00\n",
      "36 1.00\n",
      "37 1.00\n",
      "38 1.00\n",
      "39 1.00\n",
      "40 1.00\n",
      "41 1.00\n",
      "42 1.00\n",
      "43 1.00\n",
      "44 1.00\n",
      "46 1.00\n",
      "47 1.00\n",
      "48 1.00\n",
      "49 1.00\n",
      "50 1.00\n",
      "51 1.00\n",
      "52 1.00\n",
      "53 1.00\n",
      "54 1.00\n",
      "55 1.00\n",
      "56 1.00\n",
      "57 1.00\n",
      "60 1.00\n",
      "63 1.00\n",
      "67 1.00\n",
      "70 1.00\n",
      "74 1.00\n",
      "75 1.00\n",
      "84 1.00\n",
      "92 1.00\n",
      "99 1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics TODO 统计长度\n",
    "from collections import defaultdict\n",
    "len_counter = defaultdict(int)\n",
    "word_len_counter = defaultdict(int)\n",
    "num_of_words = 0\n",
    "tokenized_messages = new_messages\n",
    "tokenized_responses = new_responses\n",
    "for line in tokenized_messages + tokenized_responses:\n",
    "    len_counter[len(line)] += 1\n",
    "    num_of_words += len(line)\n",
    "    for item in line:\n",
    "        word_len_counter[len(item)] += 1\n",
    "\n",
    "sorted_length = sorted(len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/len(tokenized_messages)/2))\n",
    "\n",
    "print('char_nums_per_word:')\n",
    "sorted_length = sorted(word_len_counter.items(), key = lambda x:x[0])\n",
    "total_counter = 0\n",
    "for lens,counter in sorted_length:\n",
    "    total_counter += counter\n",
    "    print('%d %.2f' % (lens, total_counter/num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215561\n",
      "195561\n"
     ]
    }
   ],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/enword_light/'\n",
    "import random\n",
    "random.seed(6666)\n",
    "\n",
    "total_num = len(messages)\n",
    "dev_num =  10000\n",
    "test_num = 10000\n",
    "train_num = total_num - dev_num - test_num\n",
    "print(total_num)\n",
    "print(train_num)\n",
    "\n",
    "random_orders = range(0, total_num)\n",
    "messages = tokenized_messages\n",
    "responses =  tokenized_responses\n",
    "file_map = {'message':tokenized_messages, 'response':tokenized_responses}\n",
    "for file_type in file_map.keys():\n",
    "    container = file_map[file_type]\n",
    "    with open(out_path + 'dev.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(0,dev_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'test.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(dev_num,test_num + test_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))\n",
    "    with open(out_path + 'train.'+file_type,'w+',encoding='utf-8') as fout:\n",
    "        for i in range(test_num + test_num,total_num):\n",
    "            fout.write('%s\\n' % ' '.join(container[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119545\n",
      "111301\n"
     ]
    }
   ],
   "source": [
    "out_path = '/ldev/tensorflow/nmt2/nmt/data/enword_light/'\n",
    "in_path = '/ldev/tensorflow/nmt2/nmt/data/enword_light/'\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "message_counter = defaultdict(int)\n",
    "response_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for file in ['message','response']:\n",
    "    with open(in_path + 'train.'+file,'r+',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        container = message_counter\n",
    "        if file == 'response':\n",
    "            container = response_counter\n",
    "        for line in lines:\n",
    "            chars = line.strip('\\n').split(' ')\n",
    "            for char in chars:\n",
    "                if len(char) > 0:\n",
    "                    counter[char] += 1\n",
    "                    container[char] += 1\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_message_counter = sorted(message_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "sorted_response_counter = sorted(response_counter.items(), key = lambda x:x[1], reverse = True)\n",
    "print(len(sorted_message_counter))\n",
    "print(len(sorted_response_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_nums = [15000]\n",
    "for vocab_num in vocab_nums:\n",
    "    out_path = '/ldev/tensorflow/nmt2/nmt/data/enword_light/'\n",
    "    for file in ['message','response']:\n",
    "        with open(out_path + 'vocab.'+str(vocab_num)+'.'+file,'w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.response','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<UNK>\\n<S>\\n</S>\\n');\n",
    "            for item in sorted_response_counter[0:vocab_num]:\n",
    "                fout.write(item[0]+'\\n')\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.separate.message','w+',encoding='utf-8') as fout:\n",
    "            #fout.write('<unk>\\n<s>\\n</s>\\n');\n",
    "            for i,item in enumerate(sorted_message_counter[0:vocab_num]):\n",
    "                if len(item[0]) == 0:\n",
    "                    print('flag '+str(i))\n",
    "                fout.write(item[0]+'\\n')\n",
    "\n",
    "    with open(out_path + 'vocab.'+str(vocab_num)+'.counter','w+',encoding='utf-8') as fout:\n",
    "            fout.write('<unk>\\n\\t-1\\n<s>\\t-1\\n</s>\\t-1\\n');\n",
    "            for item in sorted_counter[0:vocab_num]:\n",
    "                fout.write('%s\\t%d\\n'  % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119545\n",
      "0 0.0274\n",
      "5000 0.8744\n",
      "10000 0.9159\n",
      "15000 0.9341\n",
      "20000 0.9447\n",
      "25000 0.9518\n",
      "30000 0.9572\n",
      "35000 0.9615\n",
      "40000 0.9657\n",
      "45000 0.9683\n",
      "50000 0.9704\n",
      "55000 0.9725\n",
      "60000 0.9747\n",
      "65000 0.9768\n",
      "70000 0.9789\n",
      "75000 0.9810\n",
      "80000 0.9832\n",
      "85000 0.9853\n",
      "90000 0.9874\n",
      "95000 0.9896\n",
      "100000 0.9917\n",
      "105000 0.9938\n",
      "110000 0.9959\n",
      "115000 0.9981\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    sums += counter\n",
    "print(len(sorted_message_counter))\n",
    "total_counter = 0\n",
    "for i,(lens,counter) in enumerate(sorted_message_counter):\n",
    "    total_counter += counter\n",
    "    if i%5000 == 0:\n",
    "        print('%d %.4f' % (i, total_counter/sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
